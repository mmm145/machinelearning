{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590224a1-2ee8-4344-b9f6-6de9c2c88d40",
   "metadata": {},
   "source": [
    "# Language Modeling \n",
    "\n",
    "\n",
    "## Predicting words \n",
    "\n",
    "**word prediction**\n",
    "\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Grammer or spell checking\n",
    "\n",
    "\n",
    "\n",
    "- probabilistic model of a natural language\n",
    "  - assign probability to\n",
    "    - a whole sentence\n",
    "    - each potential next word\n",
    "\n",
    "- p(w) = p(w1, w2, w3, .... wn)\n",
    "  - probability of the sentence having words, w1,w2,w3,....wn\n",
    "- P(wn / w1, w2, .... w(n-1) )\n",
    "  - p(w) or p(wn / w1, w2, w3, .... ,w(n-1))\n",
    "\n",
    "- you use conditional probability to calculate\n",
    "  - the probability of the whole sentence\n",
    "\n",
    "\n",
    "**how to calculate the probability**\n",
    "\n",
    "- join probabiity of multiple random variable\n",
    "  - p(A,B,C,D) = p(A)p(B/A)p(C/A,B)p(D/A,B,C)\n",
    "  - the order of A,B,C,D is the same as when they are in sentence\n",
    "    - \"A, B, C, D\"\n",
    " \n",
    "  - p(w1:n) = p(w1)p(w2/w1)p(w3/w1,2).....p(wn/w1:(n-1))\n",
    "    - $\\Pi_{k=1}^{n}p(wk / w1:(k-1) )$\n",
    "    \n",
    "\n",
    ">> when a sentence is too long, it is hard to calculate the whole sentence.\n",
    "   \n",
    "**Bigram Markov assumption**\n",
    "\n",
    "- we approximate the next word from\n",
    "  - last few words\n",
    "  - instead of from\n",
    "  - the whole sentence\n",
    "- p(blue/The water of Walden Pond is so beautifully) ~~ p(blue/beautifully)\n",
    "- p(wn/w1:(n-1)) ~~ p(wn/w(n-1))\n",
    "  - p(w1:n) = $\\Pi_{k=1}^{n}p(wk / w1:(k-1) )$~~$\\Pi_{k=1}^{n}p(wk/w(k-1))$\n",
    "\n",
    ">>> generally, we use the last N words.\n",
    "\n",
    "**Bag of words with N-grams**\n",
    "\n",
    "- p(wn / w1:(n-1)) ~~ p(wn / w(n-N+1):(n-1))\n",
    "\n",
    "- N = 1\n",
    "  - you predict the new word only from one last word\n",
    "- N = 2\n",
    "  - you predict the new word only from the last two words\n",
    "- N = 3\n",
    "  - you predict the new word only from the last three words\n",
    "- and so on.\n",
    "\n",
    ">> problems with N-gram models.\n",
    "\n",
    "1. can not handle the long-distance dependencies.\n",
    "2. with N-gram, it is hard to model new sequences with similar meanings.\n",
    "   - \n",
    "\n",
    ">> solution for \n",
    "- LLM\n",
    "  - can handle longer context\n",
    "  - embedding spance (??????????????) what is this and why???????/\n",
    "    - can model synonymy\n",
    "    - generate better novel strings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d366f40-c6df-47ab-9823-40d5052b6670",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "- (2010 - 2018)\n",
    "  - trying to define the better architecture\n",
    "- (2019 - 2024)\n",
    "  - we have already learned to get the best architecture so far\n",
    "  - and then\n",
    "  - do fine tune\n",
    "  - domain adaptation.\n",
    "\n",
    " # Contextual embedding\n",
    "\n",
    "- ex) the meaning of it are different, depending on the context.\n",
    "- each word should have each vector, depending of the word surrounding the word\n",
    "\n",
    "# attention mechanism\n",
    "\n",
    "- dynamically decide which part of input you should focus on.\n",
    "- NLP/Transformer/ SoTA\n",
    "- Image classification/ time series. \n",
    "\n",
    "- word attends to some negihborin words more than others.\n",
    "\n",
    "- score(xi, xj) = xi*xj\n",
    "- aij = softmax(score(xi,xj)) all j<=i\n",
    "  - similarity \n",
    "- zi = sum (j<=i) aij*xj\n",
    "\n",
    "\n",
    "**query key value**\n",
    "- Query\n",
    "  - current element being compared to the preceding inputs\n",
    "- Key\n",
    "  - preceding input that is being compared to the current element to determine a similarity.\n",
    "- Value\n",
    "  - a value of a preceding element that gets weighted and summed.\n",
    "\n",
    "\n",
    "**algorithm**\n",
    "\n",
    "- Extract query-key-value for search.\n",
    "  - x = embedded vector from input text.\n",
    "  - query = x * Wq\n",
    "  - key = x * Wk\n",
    "  - value = x * Wv\n",
    "- Compute attention weighting.\n",
    "  - Similarity or score = np.dot(Q, transepose(K))\n",
    "  - for each x, q * k\n",
    " \n",
    "  -  score / (dimension of keys)\n",
    "  -  A = softmax(Q * transpose(K) / dimension of keys )\n",
    "    - similarity between Q and K \n",
    "  -  Z = np.dot(A, V(matrix for value) )\n",
    "\n",
    "- Extract features with high attention.\n",
    "\n",
    "\n",
    "**self attention**\n",
    "- all Xs are the same for following\n",
    "- Q = XW\n",
    "- K = XW\n",
    "- V = XW\n",
    "- and, then get\n",
    "- A\n",
    "- Z\n",
    "\n",
    "- the only weights in self-attention\n",
    "  - from linear projection\n",
    "  - Wq\n",
    "  - Wk\n",
    "  - Wv\n",
    "\n",
    "- inputs of varying sequence lengths\n",
    "  - the linear projection layers ( to get query/key/value)\n",
    "    - independent of sequence length\n",
    "\n",
    "-  permutation $\\sigma$\n",
    "  -  Attention(Q, sigma(K), sigma(V) ) = Attention(Q,K,V)\n",
    "  -  Attention(sigma(Q), K, V) = sigma(Attention(Q,K,V)) \n",
    "\n",
    "\n",
    "**masked self-attention**\n",
    "\n",
    "- attention\n",
    "  - only uses\n",
    "    - past and current information\n",
    "    - mask the attention matrix\n",
    "      - so that you can not access the information from future.\n",
    "         \n",
    "- attention matrix\n",
    "  - Q * transpose(K)\n",
    "  - set upper diagonal part of attention matrix as -inf\n",
    "  - and use softmax\n",
    "  - the output will be zero\n",
    "   \n",
    "**cross-attention**\n",
    "- Q = XW\n",
    "- K = X\"W\n",
    "- V = X\"W\n",
    "- and then get\n",
    "- A\n",
    "- Z\n",
    "- from Q K\n",
    "\n",
    "\n",
    "# what happen in multi-head attention\n",
    "\n",
    "- t1(i) = LayerNorm(xi)\n",
    "- t2(i) = MultiHeadAttention(ti, [x1,...xN]\n",
    "- t3(i) = t2(i) + x(i)\n",
    "- t4(i) = LayerNorm(t3(i))\n",
    "- h(i) = t5(i) + t3(i)\n",
    "\n",
    "-  FFN(xi) = ReLU(xi W1 + b1 ) W2 + +b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feadf6d-e802-4f3f-b2d1-9bd27960a115",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "- combines the several attention heads in parallel.\n",
    "\n",
    "- MultiHead(Q, K, V) = [ head1, head2, .... headh]W\n",
    "- head(i) = Attention(QW, KW, VW)\n",
    "\n",
    "- MultiHead( Q , K, V ) = [head 1 , . . . , head h ]Wo\n",
    "\n",
    "- head(i) = Attention( Q W(i) , KW(i) , V W(i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc740c6e-8455-4b3f-82e5-7cd67dd2ed60",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- divide the text into tokens\n",
    "- tokens\n",
    "  - meaningful unit to capture following of given text\n",
    "    - semantic\n",
    "      - meaning conveyed by the given text.\n",
    "    - syntactic\n",
    "      - gramatical arrange\n",
    "- convert text data into numerical data.\n",
    "\n",
    "**ideal**\n",
    "\n",
    "- information per token should be large\n",
    "- the dictioanry size should be small.\n",
    "\n",
    "**methods**\n",
    "\n",
    "- word level tokenization\n",
    "  - space - based\n",
    "  - punctuation - based\n",
    "  - rule-based\n",
    " \n",
    "  - can lead to huge vocaburary.\n",
    " \n",
    "  - \n",
    "    \n",
    "- subword level tokenization\n",
    "- character level tokenization.\n",
    "  - Tokenized sequence are very long\n",
    "  - Individual characters do not carry a lot of mearning by themselves.\n",
    " \n",
    "\n",
    "## Subword-based tokenization\n",
    "\n",
    "- Do not split frequent words\n",
    "- Split rare words into meaningful subwords\n",
    "\n",
    "- from transformers import BertTokenizer\n",
    "- tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "- tokenizer.tokenize(\"BPE tokenization breaks text into subwords\")\n",
    "- tokenizer.encode(\"BPE tokenization breaks text into subwords\",max_length=16,padding = \"max_length\")\n",
    "  \n",
    "\n",
    "## Token embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74598b13-201f-47a4-bd27-b78dca84ec18",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
