{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dfa5c5-c50d-4b05-a0ee-4616c8610a36",
   "metadata": {},
   "source": [
    "# one type of neural network\n",
    "\n",
    "- the data is stored as graph.\n",
    "  - more complex relationship.\n",
    "  - socail networ analysis\n",
    "  - molecule analysis.\n",
    " \n",
    "## introduction\n",
    "\n",
    "**graph**\n",
    "\n",
    "- complex data structure\n",
    "  - relationship between entities\n",
    " \n",
    "- real world task as graph problems\n",
    "  - molecules as graph.\n",
    "  - social network.\n",
    "\n",
    "- label embedding\n",
    "  - eg)\n",
    "    - activity, location, gender, protein function....\n",
    "- link prediction\n",
    "  - eg)\n",
    "    - friendship recommendation, product recommendation....\n",
    "-  node embedding\n",
    "  - entity role/function comparison\n",
    "-  whole graph embedding\n",
    "  - complex data emergent property classification. \n",
    "\n",
    "**problem when using traditional machine learning**\n",
    "\n",
    "- variable size input\n",
    "  - if input size changes, which infulence the model. \n",
    "-  no natual ordering of nodes\n",
    "  - \n",
    "-  complex dependencies between nodes\n",
    "-  irrgular structure.\n",
    "\n",
    "**shallow embedding to deep embedding**\n",
    "\n",
    "\n",
    "- shallow embedding\n",
    "  - when creating new language\n",
    "    - you use alreay language that already existed as foundation of new one. \n",
    "\n",
    "- deep embedding\n",
    "  - create new langauge with new rule\n",
    "\n",
    "- limitation of shallow embedding.\n",
    "  -  1 parameter inefficiency\n",
    "    - \n",
    "  - 2 transductive only\n",
    "    - cannot create embedding for unseen data.\n",
    "  - 3 ignore node feature\n",
    "    -  do not use node attributes. \n",
    "  - 4 no structural generalization\n",
    "    - limited ability to capture complex pattern \n",
    "\n",
    "## Permutation invariance and equivariance\n",
    "\n",
    "- graph\n",
    "  - does not have canonical order of the nodes\n",
    "  - same graph can be represented by the different adjacency matrics.\n",
    " \n",
    "- machine learning models\n",
    "  - insensitive to node ordering.\n",
    "     - the input data might not have the order.\n",
    "       - if insensitive\n",
    "         - robost\n",
    "         - generalization\n",
    "- permutation invariance\n",
    "  - f(A, X ) = f(PAtranspose(P), PX)\n",
    "  - permuetes the input, the output stays the same\n",
    "  - for any permutation matrix.\n",
    "    - permutation matrix\n",
    "      - matrix for change the order of matrix\n",
    "      - element is 0 or 1\n",
    "      - all row or col has only one 1\n",
    "     \n",
    "- permutation equivariance.\n",
    "  - Pf(A, X) = f(P A transpose(P), PX)\n",
    "  - permutes the input, output also permutes accordingly. \n",
    "  - for any permutation matrix \n",
    "\n",
    "\n",
    "\n",
    "## graph neural network\n",
    "\n",
    "- it has multiple of\n",
    "  - permutation invariant functions\n",
    "  - permutation equivariant functions.\n",
    "\n",
    "## graph convolutional network (GCN)\n",
    "\n",
    "- generate\n",
    "  - node embedding\n",
    "    - node\n",
    "      - dense, low dimensional vectors (embedding)\n",
    "      - in a continous vector space\n",
    "    - aims to capture\n",
    "      - structural\n",
    "      - and\n",
    "      - semantic\n",
    "    - to represent the input as vector\n",
    "      - while maintaining the important information of graph.\n",
    "      - so that machine learning model can work\n",
    "- based on\n",
    "  - local network neighborhoods.\n",
    " \n",
    "- eg)\n",
    "  - choose one point\n",
    "  - choose k ( the number of neighboring)\n",
    "  - get all the information from k nearest neighbors\n",
    "  - aggregate all the information you got in the previous step\n",
    "  - predict graph context and label using the aggregated information.\n",
    "\n",
    "**neighborhood aggregation**\n",
    "\n",
    "- network neighborhood defines a computation graph.\n",
    "  - computation graph\n",
    "    - node\n",
    "      - operation / computation\n",
    "    - edge\n",
    "      - dependencies. \n",
    "- basic approach\n",
    "  - average the information from neighborhood\n",
    "  - and\n",
    "  - apply the neural network\n",
    " \n",
    "  -  hidden representation of node v of layer (k+1) =\n",
    "     - nonlinear function (\n",
    "       - weight matrix for neighborhood aggregation\n",
    "       - times\n",
    "       - sum of all negibors of v\n",
    "         -  hidden representation of N(v) of layer k\n",
    "         -  divided by\n",
    "         -  size of N(v)\n",
    "       -  adding\n",
    "         - weight matrix for transforming hidden vector of self\n",
    "         - inner product\n",
    "         - hidden representation of v of layer k\n",
    "\n",
    "**permutation invariance & equivariance**\n",
    "\n",
    "- GCN computation is permutaion equivariance\n",
    "  - rows of node input features == rows of output embeddings\n",
    "  - computing the embedding of a given node\n",
    "    - is invariant to neighbor ordering.\n",
    "    - after permutation,\n",
    "      - location of given node in the input\n",
    "      - changes\n",
    "      - but\n",
    "      - output embedding content\n",
    "      - stays the same.\n",
    "  - color of node feature\n",
    "    - is the same as\n",
    "    - the color of output embedding.\n",
    "\n",
    "**GCN matrix form (I wanna review 3/11 )**\n",
    "\n",
    "1. \n",
    "- q\n",
    "  - what is the meaning of each formulas. \n",
    "\n",
    "- Many aggregation can be efficiently\n",
    "  - by sparse matrix\n",
    "    - marix that has a lot of zeros\n",
    "  -  define node embedding matrix at layer k\n",
    "    - Hk = transpose(h1(k), h2(k), ....., h/v/(k))\n",
    "  - for a single node v\n",
    "    - sum on u in N(v){hu(k)}  = np.dot(A(v), H(k))\n",
    "  - normalize by degree\n",
    "    - sum on u in N(v) { h u (k) / /N(v)/}\n",
    "    -  = 1 / /N(v)/ np.dot(A(v), H(k))\n",
    "    -  = np.dot( inv(D), A)(v)\n",
    "\n",
    "2. \n",
    "- h v (k+1) =\n",
    "  - sigmoid\n",
    "    - embedding matrix k\n",
    "    - times\n",
    "    - sum u in N(v)\n",
    "      -  h u (k)\n",
    "      -  divided by\n",
    "      -  size of N(v)\n",
    "    - add\n",
    "      - wieght matrix for transforming output self\n",
    "      - inner product\n",
    "      - h v (k)\n",
    "\n",
    "- normalized adjaceny matrix\n",
    "  - norm(A) = dot( inv(D), A) )\n",
    "  - H(k+1) =\n",
    "    - sigmoid\n",
    "      - norm(A)H(k)transpose(W) + H(k)B(k))\n",
    "- norm(A)\n",
    "  - normalized A\n",
    "- H(k)\n",
    "  - embedding matrix at layer k\n",
    "- B(k)\n",
    "  - transforming matrix at layer k\n",
    "\n",
    "\n",
    "**symmetric normalization**\n",
    "\n",
    "-  without normalization\n",
    "  - the importance and scale of nodes are different\n",
    "- take a balance of each node\n",
    "\n",
    "- sym norm(A) = D^(-1/2) A D^(-1/2)\n",
    "  - D\n",
    "    - degree matrix\n",
    "    - diagonal element are the degree of each node.\n",
    "   \n",
    "\n",
    "- usually\n",
    "  - H(k+1) =\n",
    "    - sigmoid\n",
    "      - row norm(A)\n",
    "      - H(k)\n",
    "      - transpose(W)\n",
    "      - add\n",
    "      - H(k)\n",
    "      - transpose(B)\n",
    "- WW  = c(W, B)\n",
    "- =>>>\n",
    "- H(k+1) =\n",
    "  - sigmoid\n",
    "    - (\n",
    "    - row norm(A)\n",
    "    - H(k)\n",
    "    -  add\n",
    "    -  H(k)\n",
    "    - )\n",
    "    -  times\n",
    "    -  WW\n",
    "\n",
    "- AA = A + I\n",
    "- D vv = /N(v)/ + 1\n",
    "- with symmetric normalization\n",
    "\n",
    "-  H(k+1) =\n",
    "  - sigmoid\n",
    "    - D^(-1/2)\n",
    "    - AA\n",
    "    - D^(-1/2)\n",
    "    - WW \n",
    "\n",
    "\n",
    "- why symmetric normalization D^(-1/2) A D^(-1/2)\n",
    "- instead of row nomalization  D^(-1) A\n",
    "  - with row nomalization\n",
    "    - high degree node`s influence is diluted.\n",
    "  - with symmetric normalization\n",
    "    - the importance is balanced.\n",
    "\n",
    "## Graph Attention Networks (GAT)\n",
    "\n",
    "**motivation**\n",
    "\n",
    "- not all neighbors are equally import\n",
    "- the reason why we want to use GAT\n",
    "  - want to know attention weight based on node features. \n",
    "\n",
    "**key innovation**\n",
    "\n",
    "- 1. Learnable attention\n",
    "     - compute\n",
    "       - importance score\n",
    "       - between connected nodes.\n",
    "- 2. Multi-head attention\n",
    "     - multiple independent attention mechanisms for stability. \n",
    "- 3. Masked attention.\n",
    "     - only for actual neighbors.\n",
    "\n",
    "## adaptive aggregation \n",
    "\n",
    "- incorporate attention mechanism into aggregation.\n",
    "- h v (k+1) =\n",
    "  - sigmoid\n",
    "    - sum on u in N(v)\n",
    "      - attention(u,v)\n",
    "      - weight matrix(k)\n",
    "      - h u (k)\n",
    "\n",
    "-  attention(u,v)\n",
    "  - exp(e(v,u))\n",
    "  - divided by\n",
    "  - sum on k in N(v)\n",
    "    - exp( e(v,k))\n",
    "- e(v,k)\n",
    "  - LeakyReLU( transpose(a){ W (h v) // W (h k)}\n",
    "  - parameters of\n",
    "  - a\n",
    "  - are trained jointly together with weight matrices. \n",
    "\n",
    "- shape of attention\n",
    "  - like sigmoid.\n",
    "\n",
    "\n",
    "**multi-head attention in GAT**\n",
    "\n",
    "- all heads operate independently on the same input\n",
    "- different heads focus on different ascpects of data.\n",
    "\n",
    "**methods of combining heads into one**\n",
    "\n",
    "- averaging\n",
    "  - averageing before put it into sigmoid\n",
    "- concatenating\n",
    "  - concatenating after put it into sigmoid.\n",
    " \n",
    "\n",
    "## Message-Passing Neural Networks (MPNN)\n",
    "\n",
    "- unify variety of GNN architecture\n",
    "  - under general message passing framework\n",
    "\n",
    "- each node gather information from neighbor node\n",
    "  - called node embedding.\n",
    "- aggregate the message\n",
    "  - \n",
    "- update the message\n",
    "  - update function\n",
    "    - (usually) neural network\n",
    "\n",
    "- GCN in the message passing framework\n",
    "  - message function\n",
    "    - m j->i (l) = W(l)(h j (l-1)) / /N(i)/\n",
    "  - aggregation function\n",
    "    - h i (l)  =\n",
    "      - sigmoid\n",
    "        - sum on u in N(i) union {j}\n",
    "          - m j->i (l)\n",
    "         \n",
    "**GraphSAGE**\n",
    "\n",
    "- algorithm for\n",
    "  - embedding graph information to vector\n",
    "- aggregate information from neighobors.\n",
    "\n",
    "- ->> enables it to performe for the bigger data set.\n",
    "\n",
    "- traditional method is transductive\n",
    "  - from specific case to specific case\n",
    "- thanks to graphSAGE, it became inductive.\n",
    "\n",
    "**how**\n",
    "\n",
    "- sampling\n",
    "  - limit the number of nodes from neighbors.\n",
    "\n",
    "- aggregation method.\n",
    "    - mean\n",
    "    - pooling\n",
    "    - attention\n",
    "\n",
    "- deep neural net\n",
    "\n",
    "\n",
    "- h_v(l)\n",
    "  - \n",
    "\n",
    "\n",
    "## GNN training\n",
    "\n",
    "**standard GNN pipeline**\n",
    "\n",
    "- input layer\n",
    "- GNN layer\n",
    "- tasks\n",
    "  - node classification\n",
    "    - linear embedding of input \n",
    "  - link prediction\n",
    "  - graph classification.\n",
    "\n",
    "**task based on levels**\n",
    "- node level\n",
    "  - node classification\n",
    "    - predict the category of each node\n",
    "  - node regresson\n",
    "    - predict the continuous attribute of each node. \n",
    "- link level\n",
    "  - use the pair of nodes embedding\n",
    "  - link prediction\n",
    "    - predict if there is an edge between two nodes\n",
    "  - edge classification\n",
    "    - predict the category of the edge.\n",
    "  - common linke prediction method\n",
    "    - concatenate - concat(hi, hj)\n",
    "    - inner product - np.dot(hi, hj)\n",
    "    - distance based - - //hi - hj//\n",
    "    - billinaire method - hi (mat mul) W (mat mul) hj \n",
    "-  sub graph level\n",
    "-  graph level\n",
    "  - calls for aggregating all the informatoin\n",
    "  - graph classification\n",
    "    - predict the category of entire graph\n",
    "  - graph regression\n",
    "    - predict the continous property of entire graph.\n",
    " - common graph aggregation\n",
    "   - mean / sum pooling\n",
    "   - max pooling\n",
    "   - hierarchical pooling\n",
    "     - eg)\n",
    "       - Diff pool\n",
    "       - Sag pool \n",
    "   - attention pooling\n",
    "     - this algorithm get the information nodes embedding\n",
    "\n",
    "**General objective function**\n",
    "\n",
    "- all GNN can be calculated with optimization function\n",
    "  - min of set of weight L(y, f(zn))\n",
    "  - L - loss function\n",
    "  - y -\n",
    "  - f\n",
    "  - zn\n",
    "- SGD \n",
    "\n",
    "**super vised training**\n",
    "\n",
    "- for node/link/graph\n",
    "  - there is an explicit labeling. \n",
    "\n",
    "**unsuper vised training**\n",
    "\n",
    "- when explict labels are not available.\n",
    "  - graph structure can be used.\n",
    "\n",
    "- self supervised approach\n",
    "  - link prediction - predict missing edge\n",
    "  - graph reconsturuction\n",
    "    - node feature\n",
    "    - adjancy\n",
    "  - random walk.\n",
    "    - deep walk\n",
    "    - node2vec\n",
    "- contrastive learning\n",
    "  - maximize the similarity between related nodes\n",
    "  - minimize the similarity between unrelated nodes.\n",
    "- generative approach.\n",
    "  - graph generative model\n",
    "\n",
    "**inductive and transductive**\n",
    "\n",
    "\n",
    "**Receptive filed of a GNN**\n",
    "- the set of node that detemines the embedding of node.\n",
    "- the more layer, the more overlap different set of receptive filed for different node embedding.\n",
    "  - in small graph, almost all the nodes overlap the receptive fields.\n",
    "\n",
    "## inductive capability\n",
    "\n",
    "**new graphs**\n",
    "\n",
    "**new node**\n",
    "\n",
    "\n",
    "## over smoothing in GNN\n",
    "\n",
    "- after a certain amount of propagation\n",
    "  - nodes information are similar\n",
    "  - < in each update,\n",
    "    - GNN gather the information from neighbors and aggregate the information.\n",
    "- problem because of over smoothing\n",
    "  - poor model performance\n",
    "  - loss of discriminative power. \n",
    "- how to avoid over smoothing\n",
    "  - reduce hte number of layers\n",
    "  - use more efficient aggregation function.\n",
    "  - use normalization\n",
    "  - use skip connection or residual connection.\n",
    "\n",
    "**why oversmoothing happen**\n",
    "\n",
    "- the more layer, the more overlap happens between different receptive fields.\n",
    "  - the means result of embedding will be similar.\n",
    "- message passing works like averaging\n",
    "  - causing them to be less distinguishable.\n",
    "\n",
    "- so\n",
    "  - be careful to add GNN layers\n",
    "  - analyze the necessary receptive filed\n",
    "  - the required receptive field decide the number of GNN layer.\n",
    "\n",
    "**addressing oversmoothing**\n",
    "\n",
    "- what is the reason for oversmoothing?\n",
    "  - a lot of layer and more overlap of field receptive\n",
    "  -  this is something like the problem that ResNet solve for degradation\n",
    "    - with residual learning.\n",
    " - messeage passing works something like averaging.\n",
    "   -  \n",
    "\n",
    "1. skip connection\n",
    "   - F(X) + x \n",
    "2. jumping knowledge network.\n",
    "   - h v (last layer) = combine( all the output)\n",
    "     - combine - max pooling/ concatenation / LSTM attention\n",
    "3. I wanna know for dealing with messeage passing working like average.\n",
    "## graph pooling\n",
    "\n",
    "**--------**\n",
    "- to generate the compact level graph representation\n",
    "- reduce the original graph into a smaller graph structure.\n",
    "\n",
    "**--------**\n",
    "- graph classification\n",
    "\n",
    "**--------**\n",
    "- improves computational efficiency during message passing.\n",
    "- enable the hierachical abstruction\n",
    "\n",
    "**type of graph pooling**\n",
    "\n",
    "- global pooling\n",
    "  - simplest approach \n",
    "- clustering based pooling\n",
    "  - DiffPool \n",
    "- dropout based pooling.\n",
    "  - Graph U-Net\n",
    "  - SAGPool \n",
    "\n",
    "**global pooling**\n",
    "\n",
    "\n",
    "- mean\n",
    "  - given graph with node embedded\n",
    "  - mean(all node embedding in all node)\n",
    "- max\n",
    "  - max(all node embedding in all node)\n",
    "- sum pooling over all node embedding\n",
    "  - sum(all node embedding in all node)\n",
    " \n",
    "- issue\n",
    "  - these kind of graph pool lose the information of graph structure.\n",
    "- solution for this problem\n",
    "  - hierarchical graph pooling\n",
    "  - e.g) using ReLU(sum)\n",
    "    - first split the node embedding\n",
    "    - then aggregate each group with ReLU(sum)\n",
    "    - then aggregate the each outputs of ReLU(sum) from previous. \n",
    "\n",
    "**clustering based pooling**\n",
    "\n",
    "- generate the clustering assignment matrix\n",
    "- condense each cluster into a node with aggregated node attributes.\n",
    "- generate adjacency matrix for the pooled graph.\n",
    "\n",
    "**--------**\n",
    "- create clusters from node embeddings generated from GNN\n",
    "- create a single node that represents each cluster, maintaining the edge information\n",
    "\n",
    "**--------**\n",
    "\n",
    "- given\n",
    "  - S - clustering assingment matrix\n",
    "  - H - node embeddings\n",
    " \n",
    "- H (pooled) = transpose(S) * H\n",
    "- A (pooled) = transpose(S) * A * S\n",
    "\n",
    "\n",
    "**graph Unet**\n",
    "\n",
    "- adapt the u net architecture to the grpah \n",
    "- encoder\n",
    "  - series of graph convolution\n",
    "  - pooling\n",
    "    - select top k nodes based on projection score\n",
    "    - y = Xp - p is learnable projection vector\n",
    "    - get the top k nodes based on y \n",
    "- decoder\n",
    "  - series of unpooling + graph convolutinal layer\n",
    "    - place the node back to the original place.\n",
    "    - fill the missing value with\n",
    "      - 0\n",
    "      - or\n",
    "      - predicted value \n",
    "- skip connection\n",
    "  - connect decoder and encoder\n",
    "  - at the same level. \n",
    "\n",
    "**dropout based pooling**\n",
    "\n",
    "- calculate the importance score\n",
    "- select nodes based on scores.\n",
    "- reconstruct the adjacency matrix for the pooled graph.\n",
    "- provides sparser representation with important nodes.\n",
    "\n",
    "**SAGpool**\n",
    "\n",
    "**------------**\n",
    "- learnable node selection\n",
    "- first calculating the score\n",
    "- select some of them based on score. \n",
    "\n",
    "\n",
    "**------------**\n",
    "- GNN layer is used\n",
    "- to get node importannce score.\n",
    "- input\n",
    "  - node feature\n",
    "  - adjacency matrix\n",
    "- then\n",
    "  - get embedding\n",
    "  - to choose\n",
    "    - what node is more relavant to the graph structure\n",
    "\n",
    "## stacking GNN layer\n",
    "\n",
    "- when you wanna construct the deep GNN\n",
    "  - stack GNN layers sequentially.\n",
    "  - initial node features x(v) = h(v)^0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ea1a9-05b6-470c-b6f4-a74eee6a42df",
   "metadata": {},
   "source": [
    "# graph augumentation \n",
    "\n",
    "**feature augumentation**\n",
    "\n",
    "- sometimes, the feature might not be enough.\n",
    "\n",
    "- adding more useful information to the node.\n",
    "  - so the GNN can better understand and make prediction based on nodes.\n",
    "\n",
    "- input graph lacks initial features\n",
    "- certain graph structures are hard for GNN to learn\n",
    "\n",
    "- **case1 (No node features)**\n",
    "  - option A\n",
    "    - assign the same feature vector to all nodes.\n",
    "    - Pros: Simple, Inductive\n",
    "    - Cons: Limited Expressivity. \n",
    "  - option B\n",
    "    - one hot encoding\n",
    "    - assign unique ID to each node.\n",
    "    - pros: High expressivity\n",
    "    - cons: Not inductive, does not scale to laerge graph. \n",
    "- **case2 (hard to learn strucutres)**\n",
    "  - cycle length\n",
    "    - if one node is in different cycle whose length are different\n",
    "      - GNN can not tell the length difference between them.\n",
    "        - because receptive fields are the same.\n",
    "    - soluton\n",
    "      - add the length of each cycle to the node\n",
    "    - other augmentation\n",
    "      - degree distribution\n",
    "      - clustering coefficient\n",
    "      - PageRank\n",
    "      - centrality measures. \n",
    "\n",
    "**structure augumentation**\n",
    "\n",
    "- raw input grpah\n",
    "  - optimal computatoinal grpah.\n",
    "- issued\n",
    "  - too sparse - insufficient message passing\n",
    "  - too dense - computationally expensive. \n",
    "  - too large - Memory constraints.\n",
    "\n",
    "**for sparse graph**\n",
    "\n",
    "- too sparse\n",
    "\n",
    "- add edges between 2 hop node.\n",
    "  - use A + A^2\n",
    "  - instead of A\n",
    "- add virtual node\n",
    "  - add a single node to all other nodes.\n",
    "  - reduce the diameter of graph to 2.\n",
    "    - sparse means the diameter of graph is too large\n",
    "\n",
    "**for dense graph**\n",
    "\n",
    "- dense nodes can have thousandans of neighobers.\n",
    "  - processing all the neighbors can be computationally expensive.\n",
    "\n",
    "-**neighbor sampling when focusing on node**\n",
    "- sample a subset of neighbors for message passing.\n",
    "  - h v ( l + 1 )  =\n",
    "    - non linear\n",
    "      - weight matrix\n",
    "      - inner product\n",
    "      - aggregate\n",
    "        - not all the neighbors\n",
    "    - reduce\n",
    "      - computational\n",
    "      - and\n",
    "      - memory reqirements.\n",
    "    - Introduce stochasticity which can act as regularization.\n",
    "   \n",
    "\n",
    "**for large graph**\n",
    "\n",
    "- cannot fit the entire graph in GPU memory.\n",
    "  - batch training is needed.\n",
    "- **approaches**\n",
    "  - node-wise sampling\n",
    "    - Sample a batch of target nodes and extract the k hop neighborhoods. \n",
    "    - perform message passing to these subgraphs\n",
    "  - layer-wise sampling\n",
    "    - sample neighbors at each layer during forward pass.\n",
    "      - Reduces exponential growth of receptive fields. \n",
    "  - graph-wise sampling\n",
    "    - sample a couple of small subgraphs from the original graph.\n",
    "    - for each sub-graph\n",
    "      - use GNN independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5d3e8-647f-4fcd-9d87-32194ab46083",
   "metadata": {},
   "source": [
    "# Dataset splitting for graph.\n",
    "\n",
    "- graph data is not independent unlike image or text.\n",
    "  - in graph,\n",
    "    - the prediction of one node\n",
    "    - depends on\n",
    "    - the prediction of another node. \n",
    "- e.g)\n",
    "  - image 1 and image 5 in data set are independent\n",
    "  - while\n",
    "  - node 1 and node 5 in data set are connected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3b16e-af2b-4727-90e6-aa8bf992f7fa",
   "metadata": {},
   "source": [
    "# inductive and transductive learning. \n",
    "\n",
    "**transductive**\n",
    "\n",
    "- training, validation, test sets are on the same graph.\n",
    "- all nodes seen during training.\n",
    "  - some labeled\n",
    "  - some unlabeled.\n",
    "- only applicable to node/edge prediction.\n",
    "\n",
    "**inductive**\n",
    "\n",
    "- train, validation, test sets are on different graph.\n",
    "- each split contains\n",
    "  - independent graph.\n",
    "- generalization to unseen data and unseen graph.\n",
    "- application to node/edge/graph prediction.\n",
    "  - new node in dynamic graph.\n",
    "  - transfer to new graph.\n",
    "- **inductive capability**\n",
    "  - the same parameters are shared for all nodes.\n",
    "    - the number of parameters are sublinear to the number of vertext\n",
    "    - we can generalize to unseen nodes.\n",
    "  - new graphs\n",
    "    - inductive node embedding can be used for totally new data\n",
    "    - e.g)\n",
    "      - the model train the graph A\n",
    "      - then\n",
    "      - the model can be used for graph B.\n",
    "     \n",
    "**node classification**\n",
    "\n",
    "- transductive node classification.\n",
    "  -  one graph is split into train, validation, test\n",
    "- inductive node classification.\n",
    "  - multiple the whole graphs split into train , validaion,test.\n",
    "\n",
    "**graph classification**\n",
    "\n",
    "- graph level task only works when inductive learninig.\n",
    "\n",
    "**link prediction**\n",
    "\n",
    "- predict missing edges in a graph.\n",
    "- self-supervised learning.\n",
    "  - hide some edges from graph and learn if there is a edges or not.\n",
    "\n",
    "- then\n",
    "- **split edge into train/validation/test**\n",
    "  - transductive\n",
    "    - works with one graph and assign some part of it into train/validation/test dataset respectively.\n",
    "    - the whole graph is visible to all dataset split.\n",
    "    - four type of partition\n",
    "      - train message edge\n",
    "        - core graph structure visible during training. \n",
    "      - train supervision edge\n",
    "        - edges to predict during training.\n",
    "      - validation edge\n",
    "        - used for model selection.\n",
    "        - can be used many times. \n",
    "      - test edge.\n",
    "   \n",
    "    - after training\n",
    "      - the model know about the train supervision edge\n",
    "      - and\n",
    "      - use edges when testing. \n",
    "  - inductive\n",
    "    - datsets is composed of multiple independent graphs. \n",
    "    - each grap is assigned to train/validaion/testing data\n",
    "      - in each graph.\n",
    "      - decide the message edge\n",
    "      - and\n",
    "      - supervision edge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88626857-1e40-47f3-81ee-12f27a0dc40b",
   "metadata": {},
   "source": [
    "# transformation and GNN \n",
    "\n",
    "## over-squashing in message passing. \n",
    "\n",
    "- more layer in GNN\n",
    "  - the more receptive field causese overlapping\n",
    "  - the information is squashed.\n",
    "- long - range\n",
    "  - information\n",
    "    - important\n",
    "    - but\n",
    "    - far away\n",
    "      - get diluted.\n",
    "\n",
    "**why long-range matter**\n",
    "\n",
    "- e.g)\n",
    "  - Molecular structure\n",
    "    -  non - local interaction matters more than the local interaction. \n",
    "  - Social networks\n",
    "    - weak ties between communities often most informative.\n",
    "  - Knowledge graphs\n",
    "    - Reasoning requires multi hop\n",
    "- but MPGNN\n",
    "  - requires the n layer to get the information for n hop neighbors.\n",
    "- but\n",
    "  - the more layer, the more oversmoothing.\n",
    "  - the more layer, the less effective time complexity.\n",
    "- so\n",
    "  - transformer should be applied to GNN.\n",
    " \n",
    "## Graph transformers.\n",
    "\n",
    "**the benefits**\n",
    "- direct interaction between long-range node with attention.\n",
    "- parallel computation of all node pair interaction.\n",
    "- proven success in sequential data domain.\n",
    "\n",
    "**challenge to solve**\n",
    "- incorporate graph topology as inductive bias**\n",
    "-  develop positional encoding for unorderd node.\n",
    "-  integrate edge features into attention mechanism.\n",
    "\n",
    "## Graph positional encoding. \n",
    "\n",
    "- **challenges**\n",
    "  - node in a graph does not have inherent ordering.\n",
    "  - derive positional encoding from graph structure.\n",
    "\n",
    "**approach 1 (using Laplacian eigen vector)**\n",
    "\n",
    "- use eigenvectors of normalized graph laplacian.\n",
    "  - (I wanna know)\n",
    "    - why eigenvector for graph positional encoding\n",
    "    - what eigenvector and eigenvalue shows?/ what information they have for graph positional encoding??\n",
    "- properties\n",
    "  - encode the graph structure\n",
    "  - invarient to node reordering.\n",
    "  - eigen vector with small eigen values captures the global structure\n",
    "  - eigen vector with large eigen values capture the local structure.\n",
    "\n",
    "**approach 2 (Random Walk based)**\n",
    "\n",
    "- why?\n",
    "  - why we want to use random walk for positional encoding?\n",
    "- what\n",
    "  - what information \n",
    "\n",
    "- PE (i)\n",
    "  - [pii(1), pii(2), pii(3),....., pii(k)\n",
    "  - pii(k) = the probability of returning to the i th node after k step\n",
    "\n",
    "**approach 3 (shortes path)**\n",
    "- pe(i)\n",
    "  - [f ( d(i, 1) ) , f( d( i ,2 ) ) , .... f( d( i, N ) )\n",
    "  - d(i,k) - distance between node i and k\n",
    "  - f - embedding function. \n",
    "**approach 4 (Centrality Encoding)**\n",
    "\n",
    "- centrality encoding\n",
    "  - PE cent(i) = Emb(c(i))\n",
    "  - c(i) = sum j in N(i) ( 1/ dj))\n",
    "    - dj is the degree of node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f55872-9af2-4368-a54f-a9412ed01b4d",
   "metadata": {},
   "source": [
    "# incorporating \n",
    "\n",
    "- standered transformers does not have edge information.\n",
    "\n",
    "**method 1 (Multiply the edge features to the inner product of (Q, K) )**\n",
    "\n",
    "-  s(ij) =\n",
    "  - (Wq hi)W(e(ij))(Wk hj)\n",
    "  -  W(e(ij)) =  sum of  all layer eij * B(l)\n",
    "\n",
    "**method 2 (adding edge features to the inner product of (Q,K) )**\n",
    "\n",
    "- s(ij) =\n",
    "  - (Wqhi) (Wkhj) / sqrt(dk)  + MLP(e(ij))\n",
    "- MLP maps the edge feature vector to the scale.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe94f19-4f2c-4fc2-bba9-2419b33223d1",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "## Geometric Graphs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
