{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dfa5c5-c50d-4b05-a0ee-4616c8610a36",
   "metadata": {},
   "source": [
    "# one type of neural network\n",
    "\n",
    "- the data is stored as graph.\n",
    "  - more complex relationship.\n",
    "  - socail networ analysis\n",
    "  - molecule analysis.\n",
    " \n",
    "## introduction\n",
    "\n",
    "**graph**\n",
    "\n",
    "- complex data structure\n",
    "  - relationship between entities\n",
    " \n",
    "- real world task as graph problems\n",
    "  - molecules as graph.\n",
    "  - social network.\n",
    "\n",
    "- label embedding\n",
    "  - eg)\n",
    "    - activity, location, gender, protein function....\n",
    "- link prediction\n",
    "  - eg)\n",
    "    - friendship recommendation, product recommendation....\n",
    "-  node embedding\n",
    "  - entity role/function comparison\n",
    "-  whole graph embedding\n",
    "  - complex data emergent property classification. \n",
    "\n",
    "**problem when using traditional machine learning**\n",
    "\n",
    "- variable size input\n",
    "  - if input size changes, which infulence the model. \n",
    "-  no natual ordering of nodes\n",
    "  - \n",
    "-  complex dependencies between nodes\n",
    "-  irrgular structure.\n",
    "\n",
    "**shallow embedding to deep embedding**\n",
    "\n",
    "\n",
    "- shallow embedding\n",
    "  - when creating new language\n",
    "    - you use alreay language that already existed as foundation of new one. \n",
    "\n",
    "- deep embedding\n",
    "  - create new langauge with new rule\n",
    "\n",
    "- limitation of shallow embedding.\n",
    "  -  1 parameter inefficiency\n",
    "    - \n",
    "  - 2 transductive only\n",
    "    - cannot create embedding for unseen data.\n",
    "  - 3 ignore node feature\n",
    "    -  do not use node attributes. \n",
    "  - 4 no structural generalization\n",
    "    - limited ability to capture complex pattern \n",
    "\n",
    "## Permutation invariance and equivariance\n",
    "\n",
    "- graph\n",
    "  - does not have canonical order of the nodes\n",
    "  - same graph can be represented by the different adjacency matrics.\n",
    " \n",
    "- machine learning models\n",
    "  - insensitive to node ordering.\n",
    "     - the input data might not have the order.\n",
    "       - if insensitive\n",
    "         - robost\n",
    "         - generalization\n",
    "- permutation invariance\n",
    "  - f(A, X ) = f(PAtranspose(P), PX)\n",
    "  - permuetes the input, the output stays the same\n",
    "  - for any permutation matrix.\n",
    "    - permutation matrix\n",
    "      - matrix for change the order of matrix\n",
    "      - element is 0 or 1\n",
    "      - all row or col has only one 1\n",
    "     \n",
    "- permutation equivariance.\n",
    "  - Pf(A, X) = f(P A transpose(P), PX)\n",
    "  - permutes the input, output also permutes accordingly. \n",
    "  - for any permutation matrix \n",
    "\n",
    "\n",
    "\n",
    "## graph neural network\n",
    "\n",
    "- it has multiple of\n",
    "  - permutation invariant functions\n",
    "  - permutation equivariant functions.\n",
    "\n",
    "## graph convolutional network (GCN)\n",
    "\n",
    "- generate\n",
    "  - node embedding\n",
    "    - node\n",
    "      - dense, low dimensional vectors (embedding)\n",
    "      - in a continous vector space\n",
    "    - aims to capture\n",
    "      - structural\n",
    "      - and\n",
    "      - semantic\n",
    "    - to represent the input as vector\n",
    "      - while maintaining the important information of graph.\n",
    "      - so that machine learning model can work\n",
    "- based on\n",
    "  - local network neighborhoods.\n",
    " \n",
    "- eg)\n",
    "  - choose one point\n",
    "  - choose k ( the number of neighboring)\n",
    "  - get all the information from k nearest neighbors\n",
    "  - aggregate all the information you got in the previous step\n",
    "  - predict graph context and label using the aggregated information.\n",
    "\n",
    "**neighborhood aggregation**\n",
    "\n",
    "- network neighborhood defines a computation graph.\n",
    "  - computation graph\n",
    "    - node\n",
    "      - operation / computation\n",
    "    - edge\n",
    "      - dependencies. \n",
    "- basic approach\n",
    "  - average the information from neighborhood\n",
    "  - and\n",
    "  - apply the neural network\n",
    " \n",
    "  -  hidden representation of node v of layer (k+1) =\n",
    "     - nonlinear function (\n",
    "       - weight matrix for neighborhood aggregation\n",
    "       - times\n",
    "       - sum of all negibors of v\n",
    "         -  hidden representation of N(v) of layer k\n",
    "         -  divided by\n",
    "         -  size of N(v)\n",
    "       -  adding\n",
    "         - weight matrix for transforming hidden vector of self\n",
    "         - inner product\n",
    "         - hidden representation of v of layer k\n",
    "\n",
    "**permutation invariance & equivariance**\n",
    "\n",
    "- GCN computation is permutaion equivariance\n",
    "  - rows of node input features == rows of output embeddings\n",
    "  - computing the embedding of a given node\n",
    "    - is invariant to neighbor ordering.\n",
    "    - after permutation,\n",
    "      - location of given node in the input\n",
    "      - changes\n",
    "      - but\n",
    "      - output embedding content\n",
    "      - stays the same.\n",
    "  - color of node feature\n",
    "    - is the same as\n",
    "    - the color of output embedding.\n",
    "\n",
    "**GCN matrix form (I wanna review 3/11 )**\n",
    "\n",
    "1. \n",
    "- q\n",
    "  - what is the meaning of each formulas. \n",
    "\n",
    "- Many aggregation can be efficiently\n",
    "  - by sparse matrix\n",
    "    - marix that has a lot of zeros\n",
    "  -  define node embedding matrix at layer k\n",
    "    - Hk = transpose(h1(k), h2(k), ....., h/v/(k))\n",
    "  - for a single node v\n",
    "    - sum on u in N(v){hu(k)}  = np.dot(A(v), H(k))\n",
    "  - normalize by degree\n",
    "    - sum on u in N(v) { h u (k) / /N(v)/}\n",
    "    -  = 1 / /N(v)/ np.dot(A(v), H(k))\n",
    "    -  = np.dot( inv(D), A)(v)\n",
    "\n",
    "2. \n",
    "- h v (k+1) =\n",
    "  - sigmoid\n",
    "    - embedding matrix k\n",
    "    - times\n",
    "    - sum u in N(v)\n",
    "      -  h u (k)\n",
    "      -  divided by\n",
    "      -  size of N(v)\n",
    "    - add\n",
    "      - wieght matrix for transforming output self\n",
    "      - inner product\n",
    "      - h v (k)\n",
    "\n",
    "- normalized adjaceny matrix\n",
    "  - norm(A) = dot( inv(D), A) )\n",
    "  - H(k+1) =\n",
    "    - sigmoid\n",
    "      - norm(A)H(k)transpose(W) + H(k)B(k))\n",
    "- norm(A)\n",
    "  - normalized A\n",
    "- H(k)\n",
    "  - embedding matrix at layer k\n",
    "- B(k)\n",
    "  - transforming matrix at layer k\n",
    "\n",
    "\n",
    "**symmetric normalization**\n",
    "\n",
    "-  without normalization\n",
    "  - the importance and scale of nodes are different\n",
    "- take a balance of each node\n",
    "\n",
    "- sym norm(A) = D^(-1/2) A D^(-1/2)\n",
    "  - D\n",
    "    - degree matrix\n",
    "    - diagonal element are the degree of each node.\n",
    "   \n",
    "\n",
    "- usually\n",
    "  - H(k+1) =\n",
    "    - sigmoid\n",
    "      - row norm(A)\n",
    "      - H(k)\n",
    "      - transpose(W)\n",
    "      - add\n",
    "      - H(k)\n",
    "      - transpose(B)\n",
    "- WW  = c(W, B)\n",
    "- =>>>\n",
    "- H(k+1) =\n",
    "  - sigmoid\n",
    "    - (\n",
    "    - row norm(A)\n",
    "    - H(k)\n",
    "    -  add\n",
    "    -  H(k)\n",
    "    - )\n",
    "    -  times\n",
    "    -  WW\n",
    "\n",
    "- AA = A + I\n",
    "- D vv = /N(v)/ + 1\n",
    "- with symmetric normalization\n",
    "\n",
    "-  H(k+1) =\n",
    "  - sigmoid\n",
    "    - D^(-1/2)\n",
    "    - AA\n",
    "    - D^(-1/2)\n",
    "    - WW \n",
    "\n",
    "\n",
    "- why symmetric normalization D^(-1/2) A D^(-1/2)\n",
    "- instead of row nomalization  D^(-1) A\n",
    "  - with row nomalization\n",
    "    - high degree node`s influence is diluted.\n",
    "  - with symmetric normalization\n",
    "    - the importance is balanced.\n",
    "\n",
    "## Graph Attention Networks (GAT)\n",
    "\n",
    "**motivation**\n",
    "\n",
    "- not all neighbors are equally import\n",
    "- the reason why we want to use GAT\n",
    "  - want to know attention weight based on node features. \n",
    "\n",
    "**key innovation**\n",
    "\n",
    "- 1. Learnable attention\n",
    "     - compute\n",
    "       - importance score\n",
    "       - between connected nodes.\n",
    "- 2. Multi-head attention\n",
    "     - multiple independent attention mechanisms for stability. \n",
    "- 3. Masked attention.\n",
    "     - only for actual neighbors.\n",
    "\n",
    "## adaptive aggregation \n",
    "\n",
    "- incorporate attention mechanism into aggregation.\n",
    "- h v (k+1) =\n",
    "  - sigmoid\n",
    "    - sum on u in N(v)\n",
    "      - attention(u,v)\n",
    "      - weight matrix(k)\n",
    "      - h u (k)\n",
    "\n",
    "-  attention(u,v)\n",
    "  - exp(e(v,u))\n",
    "  - divided by\n",
    "  - sum on k in N(v)\n",
    "    - exp( e(v,k))\n",
    "- e(v,k)\n",
    "  - LeakyReLU( transpose(a){ W (h v) // W (h k)}\n",
    "  - parameters of\n",
    "  - a\n",
    "  - are trained jointly together with weight matrices. \n",
    "\n",
    "- shape of attention\n",
    "  - like sigmoid.\n",
    "\n",
    "\n",
    "**multi-head attention in GAT**\n",
    "\n",
    "- all heads operate independently on the same input\n",
    "- different heads focus on different ascpects of data.\n",
    "\n",
    "**methods of combining heads into one**\n",
    "\n",
    "- averaging\n",
    "  - averageing before put it into sigmoid\n",
    "- concatenating\n",
    "  - concatenating after put it into sigmoid.\n",
    " \n",
    "\n",
    "## Message-Passing Neural Networks (MPNN)\n",
    "\n",
    "- unify variety of GNN architecture\n",
    "  - under general message passing framework\n",
    "\n",
    "- each node gather information from neighbor node\n",
    "  - called node embedding.\n",
    "- aggregate the message\n",
    "  - \n",
    "- update the message\n",
    "  - update function\n",
    "    - (usually) neural network\n",
    "\n",
    "- GCN in the message passing framework\n",
    "  - message function\n",
    "    - m j->i (l) = W(l)(h j (l-1)) / /N(i)/\n",
    "  - aggregation function\n",
    "    - h i (l)  =\n",
    "      - sigmoid\n",
    "        - sum on u in N(i) union {j}\n",
    "          - m j->i (l)\n",
    "         \n",
    "**GraphSAGE**\n",
    "\n",
    "- algorithm for\n",
    "  - embedding graph information to vector\n",
    "- aggregate information from neighobors.\n",
    "\n",
    "- ->> enables it to performe for the bigger data set.\n",
    "\n",
    "- traditional method is transductive\n",
    "  - from specific case to specific case\n",
    "- thanks to graphSAGE, it became inductive.\n",
    "\n",
    "**how**\n",
    "\n",
    "- sampling\n",
    "  - limit the number of nodes from neighbors.\n",
    "\n",
    "- aggregation method.\n",
    "    - mean\n",
    "    - pooling\n",
    "    - attention\n",
    "\n",
    "- deep neural net\n",
    "\n",
    "\n",
    "- h_v(l)\n",
    "  - \n",
    "\n",
    "\n",
    "## GNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f97c62-bd09-4470-a6b7-09bf64ebe5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
