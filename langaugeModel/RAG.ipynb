{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a740914a-1026-414e-91b3-c19aebf70469",
   "metadata": {},
   "source": [
    "# def\n",
    "\n",
    "- retrieval augmented generation\n",
    "\n",
    "- retrieval language model\n",
    "  - that uses data store when test time. \n",
    "\n",
    "**steps**\n",
    "\n",
    "- search information based on user s instruction\n",
    " - vector search\n",
    "   - understand the meaning of word and get the related information. \n",
    " - key word search\n",
    "   - understand the pattern of word or strings, and get the information that is similar to the information. \n",
    "- LLM understands the result and answer based on user s instruction.\n",
    "\n",
    "**difference between LLM and RAG**\n",
    "\n",
    "- llm\n",
    "  - you only can use information that is public\n",
    "  - usually, it is black box and you can not know the inner side of model\n",
    "- rag\n",
    "  - you can use information that is not public\n",
    "  - you use rag and get information\n",
    "  - then\n",
    "  - you can use LLM to communicate efficiently.\n",
    "\n",
    "  - the model can get information from text collection. \n",
    " \n",
    "**how you can use RAG**\n",
    "\n",
    "- inqueris response\n",
    "  - with rag, the model can use the external datastore, which leads to proper response to the question.\n",
    "- analytics work\n",
    "  - ex)\n",
    "    - get the information of what the customer bought and behaved.\n",
    "    - get the information about the market and make it possible to analyze the market.\n",
    "- information search\n",
    "  - without rag\n",
    "    - you have to check each folder\n",
    "  - with rag\n",
    "    - you do not have to check the rag\n",
    "- generate contents.\n",
    "  - with traditional generative AI\n",
    "    - you can only use the information that is public to generate the content\n",
    "  - with RAG\n",
    "    - you can use the information that is not public.\n",
    "   \n",
    "\n",
    "## combination of llm and rag\n",
    "\n",
    "**benefits**\n",
    "\n",
    "- to improve the accuracy of information\n",
    "  - you can get the variety of information with RAG and LLM \n",
    "- easier update of information\n",
    "  - llm\n",
    "    - requires fine tuning to learn the information\n",
    "  - rag\n",
    "    - the model use the external information the user arranged before. \n",
    "- personalize the answers\n",
    "  -  LLM\n",
    "    - the model results are based on the input of user\n",
    "  -  RAG\n",
    "    - the model can use the external information \n",
    "- deal with the hullcination\n",
    "  - hullcination\n",
    "    - the generative AI creates the wrong information\n",
    "  - LLM\n",
    "    - depends on whether the input information is correct or not\n",
    "  - RAG\n",
    "    - can use the correct external information.\n",
    "\n",
    "\n",
    "\n",
    "# the type of retrieval method \n",
    "\n",
    "in retrieval process, model calculate the similarity and get the similar document to query from data store. \n",
    "\n",
    "- sparse retrieval\n",
    "  - sim(i,j) = ( freaquencey of term i in document j ) * log ( number of all document  / the number of document containing term i)\n",
    "  - using angle instead of distance\n",
    "    - ex)\n",
    "    - when make the length of document twice as the original\n",
    "    - the similarity should be the same but if the model uses the distance.\n",
    "    - It detects the distance. \n",
    "- dense retrieval\n",
    "  - sim(i,j) = encoder(i) * encoder(j)\n",
    "    - encode from query\n",
    "    - encode from data store\n",
    "    - and then, use the kNN. \n",
    "    - end to end learning\n",
    "\n",
    "## sparse retrieval \n",
    "\n",
    "set up so you want to make data set into this form.\n",
    "\n",
    "- transform document into word space and set the weight for each word\n",
    "  - dj = (w1j, w2j, ...... wnj)\n",
    "    - wij is the weight of term i in document j\n",
    "  - dq = (w1q, w2q, ...... wtq)\n",
    "\n",
    "### similarity \n",
    "\n",
    "- cos similairty between query and document\n",
    "  - sim(i, j) =  cos(theta) = np.dot(dj, q) / (abs(dj)*abs(q)) = sum(wij * wiq) / (abs(dj) * abs(q) )\n",
    "  - why you use angle similarity?\n",
    "    - ex)\n",
    "    - when you make length of sring twice as original one.\n",
    "    - the similarity should be the same but the distance is different.\n",
    "   \n",
    "    -  angle\n",
    "      - focuses more on distribution pattern\n",
    "    - distance\n",
    "      - focuses more on the length of document\n",
    "- partial matching\n",
    "  - similairty between query and documents can be partial\n",
    " \n",
    "\n",
    "### the way to get weight\n",
    "\n",
    "**two propaty that weight should have**\n",
    "\n",
    "- similarity\n",
    "  - term frequency within document\n",
    "  - non normalize\n",
    "  - normalized\n",
    "    - tf(i,j) / max ( t(ii, j)) \n",
    "- dissimilarity\n",
    "  -  idf\n",
    "    - inverse document frequency\n",
    "    - document freaquecy\n",
    "      - the number of document containing the term i\n",
    "  - shows how rare the term is\n",
    "    -  idf(t) = log ( the number of all documents / the number of documents containing the term t)\n",
    "\n",
    "**weight**\n",
    "\n",
    "- document\n",
    "  - tf(t) * idf(t)\n",
    "- query\n",
    "  - (0.5 + 0.5 * tf(t) ) * idf(t)\n",
    "  - query is usually shoter\n",
    "    - some term is 0 or 1 for term factor\n",
    "    - this is something like\n",
    "      - imbalance in categorical variable\n",
    "      - outlier in numerical variable\n",
    "\n",
    "**sparse vector to index**\n",
    "\n",
    "- given\n",
    "  - matrix\n",
    "  - row - each term\n",
    "  - col - each document\n",
    "  - element - count of each term in each document.\n",
    "- convert to\n",
    "- json\n",
    "  - key - each term\n",
    "  - value - the document containing the term of value\n",
    "    - sort them with the number of each term.\n",
    "\n",
    " **BM(25)**\n",
    "\n",
    " - improve\n",
    "   - idf\n",
    "   - by\n",
    "   - factoring with\n",
    "     - tf and length of document.\n",
    "\n",
    "   - BM(q, d)\n",
    "     - sum through t in q\n",
    "       - IDF(t)\n",
    "       - times\n",
    "         - ( k1 + 1 ) * tf (t, d)\n",
    "         - divided by \n",
    "         - k1 * ( ( 1 - b ) + b ( len(d) / average length )) + tf( t, d )\n",
    "\n",
    "- k1\n",
    "  - parameter for tuning\n",
    "    - control term frequency\n",
    "    - higher. more importance to frequency\n",
    "    - lower. more binary scoring. \n",
    "- b\n",
    "  - parameter for length.\n",
    "  - typically. 0.75\n",
    "  - b = 0\n",
    "    - No length normalization. \n",
    "  - b = 1\n",
    "    - Full length normalization.  \n",
    "           \n",
    "**advantage of sparse vector**\n",
    "\n",
    "- simple and efficient to compute\n",
    "  - no training\n",
    "  - no parameter tuning\n",
    "  - can handle a lot of documents by scaling.\n",
    "- works with variety type of text\n",
    "  - documents are scaled with continous rather than binary.\n",
    "  - score reflects\n",
    "    - quality\n",
    "    - and\n",
    "    - quantity.\n",
    "**limitation of sparse vector**\n",
    "\n",
    "- assumption of feature variable being independence.\n",
    "  - each dimension in a vector space will be treated independently.\n",
    "  -  ex)\n",
    "    - \"New\" \"York\" instead of \"New York\"\n",
    "- possibility of the model missing the semantic\n",
    "  - depends only on lexical\n",
    "    - noun\n",
    "    - verb\n",
    "    - ajective\n",
    "  - can not detect the word that has the same meaning with the different spell\n",
    "    - car\n",
    "    - automobile\n",
    "  - can not handle the word that have the same spell but different meaning.\n",
    "- order and proxiy\n",
    "  - bag of word\n",
    "      - get the information of frequency of each word\n",
    "      -  so\n",
    "      -  the sparse retrieval can not detect the difference\n",
    "      -  between\n",
    "      -  \"A causes B\"\n",
    "      -  and\n",
    "      -  \"B causes A\"\n",
    "    \n",
    "\n",
    "## dense retrieval \n",
    "\n",
    "**def**\n",
    "\n",
    "- encode query\n",
    "- and\n",
    "- encode datastore\n",
    "  - then\n",
    "  - implement\n",
    "  - KNN \n",
    "  \n",
    "- end to end learning\n",
    "  - map\n",
    "    - similar content\n",
    "    - to\n",
    "    - similar vector\n",
    "- no manual engineering needed.\n",
    "\n",
    "**basic architecture**\n",
    "\n",
    "- query encoding\n",
    "  - q is incoming query\n",
    "  - q = MLP(BERT(q))\n",
    "- document encoding\n",
    "  - D is document corpus. \n",
    "  - d = MLP(BERT(q))\n",
    "- similarity scoring.\n",
    "  - np.dot( transepose(q), d )\n",
    "  - top(k, corpus) =  argmax( transpose(q) * Corpus, k)\n",
    " \n",
    "## loss function\n",
    "\n",
    "- InfoNCE\n",
    "  - negative\n",
    "  - log\n",
    "    - exp( f( q, d+ ) )\n",
    "    - divided by\n",
    "    - exp( f( q, d+ ) ) + sum of negative sampling (exp ( f(q, d-) ) )\n",
    "   \n",
    "- f(q, d+)\n",
    "    - relevant query\n",
    "    - document pairs ( given )\n",
    "\n",
    "- negative sampling\n",
    "  - random negative sampling will fail\n",
    "  - we want to get the negative document from corpus\n",
    "  - but\n",
    "  -  most of the documents is so irrelevant that the model can not learn\n",
    "  - ~> we want to get the hard negative\n",
    "    - hard negative\n",
    "      - document similar to positive but negative.\n",
    "\n",
    "## Dense Passage Retrieval (DPR) \n",
    "\n",
    "**def**\n",
    "\n",
    "- learn encoders based on a BM 25 hard negatives and in-batch negatives.\n",
    "\n",
    "**purpose**\n",
    "\n",
    "- train encoder to detect the similarity between passage and question in open domain answering question system.\n",
    "- this is important when using the big corpus.\n",
    "\n",
    "**positive sample**\n",
    "\n",
    "- passage. question and the answr\n",
    "- passage. question and passage whose BM25 score is high. \n",
    "\n",
    "**negative sample**\n",
    "\n",
    "- random passage that are not related to question.\n",
    "- hard negative\n",
    "  - high BM25 - means related to the question more\n",
    "  - but\n",
    "  - does not have the answer.\n",
    "  - passage that has the answer for the other question.\n",
    "\n",
    "\n",
    "**how to search**\n",
    "\n",
    "- query arrive\n",
    "- calculate the similarity with all the document\n",
    "\n",
    "- cons\n",
    "  - time complexity\n",
    "    - u have to calculate a lot of things\n",
    "  - space complexity\n",
    "    - if the Dense passage retrieval uses big corpus.\n",
    "    - the memory cannot store all vectors in memory.\n",
    "\n",
    "**solutions for this con**\n",
    "\n",
    "- ANNS ( Approximate nearest neibors search ) \n",
    "  - low accuracy but faster\n",
    "  - little bit lower than usual one.\n",
    " \n",
    "  -  formula for ANNS\n",
    "    - // q - d' // <= ( 1 + epsilon ) // q - d* //\n",
    "      - q- query\n",
    "      - d'-document you will find from corpus\n",
    "      - d*-actual nearest neighobers.\n",
    "     \n",
    "**MIPS (maximum inner product search)**\n",
    "\n",
    "- store embedding representaion in data base\n",
    "- to make faster retrieval possible.\n",
    " - embedding representation in data base\n",
    " - is more organized than\n",
    " - the data in corpus\n",
    "- try to get the embedding representaion in data base\n",
    "  - that maximize the inner product between\n",
    "    - embedded vector.\n",
    "    - and\n",
    "    - query vector.\n",
    "- using ANNS (approximation nearest neighober search)\n",
    "\n",
    "\n",
    "**Mips Algorithm**\n",
    "\n",
    "- Locality-Sensitive hashing.\n",
    "  - LSH\n",
    "  - define hash functions sensitive to similarity\n",
    "  - map input vectors to bucket.\n",
    "  - hash similar items to same buckets.\n",
    "  - search only within bucket.\n",
    "- Facebook AI similarity search (FAISS)\n",
    "- for big data\n",
    "- vector quantization\n",
    "- clustering\n",
    "  - Assumption\n",
    "    - high dimensional distance follow gaussian distribution.\n",
    "  - Implementation\n",
    "    - divide / split space vector into clusters.\n",
    "    - vector quatinzation\n",
    "      - Coarse quantization for cluster selection\n",
    "        - find cluster\n",
    "      - Fine quantization with in cluster\n",
    "        - search data in the cluster.\n",
    "- Hierarchical Navigable Small World.\n",
    "  - structure\n",
    "    - layered vector\n",
    "      - from top\n",
    "      - to bottom\n",
    "      - the lower the layer, the more dot the layer has.\n",
    "  - implementation\n",
    "    - start from top\n",
    "    - in each layer,\n",
    "      - go to as near as possible to the target.\n",
    "     \n",
    "**advantage of dense retrieval**\n",
    "\n",
    "- learn semantic relationships\n",
    "- handle paraphrasing/synonys\n",
    "- language-agnostic capability\n",
    "- improved accuracy through training.\n",
    "\n",
    "**challenge of dense retrieval**\n",
    "\n",
    "- high computational cost\n",
    "- requires large training data.\n",
    "- ANNS overhead for large collection\n",
    "- less interpretable than sparce retrieval.\n",
    "\n",
    "\n",
    "# methods \n",
    "\n",
    "## retrieval reader model \n",
    "\n",
    "- open domain\n",
    "  - answer quetion.\n",
    "\n",
    "- 1. retrieve related document\n",
    "  2. read the document and answer the quetions.\n",
    "\n",
    "- retriever\n",
    "  - Dense passage retriever\n",
    "  - BM25 \n",
    "- reader\n",
    "  - Bert\n",
    "  - T5\n",
    "    - to generate the answer from the passages from retriever.\n",
    "**advantage**\n",
    "- easy\n",
    "  - just combine the reader and retriever\n",
    "- be able to use pre-trained model.\n",
    "- can be applied to many cases\n",
    "\n",
    "**challenges**\n",
    "\n",
    "- use pre-trained model\n",
    "  - you can not custermize retrieval and reader model for each case\n",
    "- limit of context\n",
    "  - the information should be related well.\n",
    "  - otherwise\n",
    "  - reader can not output the best answer.\n",
    "\n",
    "## REALM (Retrieval Language Augmented Model)\n",
    "\n",
    "- information search\n",
    "- and\n",
    "- natural language processing.\n",
    "\n",
    "- retrieval\n",
    "  - dense passage retriever\n",
    "    - encoder(query) * encoder(document)\n",
    "    - then do\n",
    "    - KNN\n",
    "  - BM 25\n",
    "- reader\n",
    "  - BERT\n",
    "  - T5\n",
    " \n",
    "**training and how to create index**\n",
    "\n",
    "- index\n",
    "  - document or some frequency data\n",
    "  - to\n",
    "  - json (data base looks like dictionary)\n",
    " \n",
    "## RAG\n",
    "\n",
    "- combined\n",
    "  - trained retriever\n",
    "  - and\n",
    "  - autoregressive BART (starting from pretrained BART)\n",
    "- generative\n",
    "\n",
    "## Retrival in context LM model\n",
    "\n",
    "- de\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58203f8e-1e12-4ff5-81df-230d3b3a9758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
