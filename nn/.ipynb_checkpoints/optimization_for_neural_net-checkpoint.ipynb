{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13abd7f8-55b9-4482-8182-53a25a16a3a2",
   "metadata": {},
   "source": [
    "# type of optimization.\n",
    "\n",
    "\n",
    "**optimization algorithm**\n",
    "\n",
    "**first order**\n",
    "- Gradient Descent\n",
    "  - new parameter = old parameter - (learning rate)* (gradient of loss with regard to parameter)\n",
    "- Stochastic Gradient Descent (SGD) / batch size = 1 Mini Batch Gradient Descent.\n",
    "  - when calculating gradient, you pick pu one observation from all observation data.\n",
    "- Mini Batch Stochastic Gradient Descent (MB-SGD)\n",
    "- momentume methods\n",
    "    - SGD with momentum\n",
    "      -  think about the gradient history\n",
    "      -  v =  learning rate * v - alpha * gradient with regard to w of m(mini batch) loss\n",
    "      -  w = v + w\n",
    "    - SGD with Nesterov momentum\n",
    "      - another momentum\n",
    "      - v = momentum * v - learning rate * gradient(w + momentum * v)\n",
    "      - w = w + v\n",
    "      - or\n",
    "      - vv = v\n",
    "      - v = momentum * v - learning rate * gradient\n",
    "      - w = w + v - momentum * vv\n",
    "      - or\n",
    "      - v = momentum * v - learning rate * gradient\n",
    "      - w = w + momentum * v - learning rate * gradient.\n",
    "      -  SGD with nesterov momentum > SGD with momentum > SGD \n",
    "    - Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "\n",
    "- Adaptive method\n",
    "- the learning rate is variable\n",
    "- ensured to converge\n",
    "- input sample is not linearly separable. \n",
    "    - Adaptive Gradient (AdaGrad)\n",
    "      - not work well when\n",
    "        - loss function is not convex\n",
    "        - gradients are dense because of rapid decay \n",
    "    - AdaDelta\n",
    "    - RMSprop\n",
    "      - to mitigate the rapid change of direction.\n",
    "      - modify a size of learning rate based on a size of gradient.\n",
    "    - Adam\n",
    "      - RMSprop + momentum\n",
    "      - mdw = gamma1 * mdw + (1 - gamma1) * dw\n",
    "      - mdb = gamma1 * mdb + (1 - gamma1) * db\n",
    "      - vdw = gamma2 * vdw + (1 - gamma2) * dw ** 2\n",
    "      - vdb = gamma2 * vdb + (1 - gamma2) * db ** 2\n",
    "      - mdw_corr = mdw / (1 - np. power (gammal, currentepoch + 1))\n",
    "      - mdb_corr = mdb / (1 - np. power (gammal, currentepoch + 1))\n",
    "      - vdw_corr = vdw / (1 - np. power (gamma2, currentepoch + 1))\n",
    "      - vdb_corr = vdb / (1 - np. power (gamma2, currentepoch + 1)) model. layers [index].weight -= (alpha / (np.sgrt (vdw_corr + 1e-08))) * mdw_corr\n",
    "      - model. layers [index].bias -= (alpha / (np.sqrt (vdb_corr + 1e-08))) * mdb_corr\n",
    "\n",
    "**second order**\n",
    "\n",
    "- newton method\n",
    "- conjugate gradient\n",
    "- quasi-newton gradient\n",
    "- Levenberg-Marquardt algorithm. \n",
    "\n",
    "\n",
    "https://neptune.ai/blog/deep-learning-optimization-algorithms\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent \n",
    "\n",
    "**algorithm**\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161adbbc-b09a-48ae-8deb-f3120845f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdf8e607-b3f0-41ca-ac87-15dffb98ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimization:\n",
    "    def __init__(self,lr=0.01,max_itr=100,tol=1e-5):\n",
    "        self.lr = lr\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        \n",
    "    def GD(self):\n",
    "        \"\"\" Gradient Descent function \"\"\"\n",
    "        print(\"gradient descent\")\n",
    "        return\n",
    "    \n",
    "    def sgd_momentum(w, dw, config=None):\n",
    "        \n",
    "        \"\"\"\n",
    "          Performs stochastic gradient descent with momentum.\n",
    "    \n",
    "          config format:\n",
    "          - learning_rate: Scalar learning rate.\n",
    "          - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "            Setting momentum = 0 reduces to sgd.\n",
    "          - velocity: A numpy array of the same shape as w and dw used to store a moving\n",
    "            average of the gradients.\n",
    "        \"\"\"\n",
    "        \n",
    "        if config is None: \n",
    "            config = {}\n",
    "        config.setdefault('learning_rate', 1e-2)\n",
    "        config.setdefault('momentum', 0.9) # set momentum to 0.9 if it wasn't there\n",
    "        v = config.get('velocity', np.zeros_like(w))\t # gets velocity, else sets it to zero.\n",
    "        v = config[\"momentum\"]*v - config[\"learning_rate\"]*dw\n",
    "        w = w + v \n",
    "        config['velocity'] = v\n",
    "    \n",
    "        return next_w, config\n",
    "        \n",
    "    def sgd_nesterov_momentum(w, dw, config=None):\n",
    "      \"\"\"\n",
    "      Performs stochastic gradient descent with Nesterov momentum.\n",
    "    \n",
    "      config format:\n",
    "      - learning_rate: Scalar learning rate.\n",
    "      - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "        Setting momentum = 0 reduces to sgd.\n",
    "      - velocity: A numpy array of the same shape as w and dw used to store a moving\n",
    "        average of the gradients.\n",
    "      \"\"\"\n",
    "      if config is None: \n",
    "        config = {}\n",
    "      config.setdefault('learning_rate', 1e-2)\n",
    "      config.setdefault('momentum', 0.9) # set momentum to 0.9 if it wasn't there\n",
    "      v = config.get('velocity', np.zeros_like(w))\t # gets velocity, else sets it to zero.\n",
    "\n",
    "      v = config[\"momentum\"]*v - config[\"learning_rate\"]*dw\n",
    "      next_w = w + config[\"momentum\"]*v - config[\"learning_rate\"]*dw\n",
    "        \n",
    "      config['velocity'] = v\n",
    "    \n",
    "      return next_w, config\n",
    "    \n",
    "    def rmsprop(w, dw, config=None):\n",
    "      \"\"\"\n",
    "      Uses the RMSProp update rule, which uses a moving average of squared gradient\n",
    "      values to set adaptive per-parameter learning rates.\n",
    "    \n",
    "      config format:\n",
    "      - learning_rate: Scalar learning rate.\n",
    "      - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n",
    "        gradient cache.\n",
    "      - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "      - a: Moving average of second moments of gradients.\n",
    "      \"\"\"\n",
    "      if config is None: config = {}\n",
    "      config.setdefault('learning_rate', 1e-2)\n",
    "      config.setdefault('decay_rate', 0.99)\n",
    "      config.setdefault('epsilon', 1e-8)\n",
    "      config.setdefault('a', np.zeros_like(w))\n",
    "    \n",
    "      next_w = None\n",
    "\n",
    "      lr = config[\"learning_rate\"]\n",
    "      dr = config[\"decay_rate\"]\n",
    "      e = config[\"epsilon\"]\n",
    "    \n",
    "        \n",
    "      return next_w, config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
