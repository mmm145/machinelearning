{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf108a52-240f-4841-a038-718c216eda26",
   "metadata": {},
   "source": [
    "# initialization \n",
    "\n",
    "- random small\n",
    "- random large\n",
    "- Xavier initialization\n",
    "  - activation function that is symmetric around 0\n",
    "    - sigmoid\n",
    "    - tanh\n",
    "  - with transformer as well\n",
    "- He(MSRA) initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e28926-bfb9-4596-b718-96e28ebcb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ba9cc-b0fa-415a-b721-5a37f6f6d0b3",
   "metadata": {},
   "source": [
    "# small random with initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c9a464a-a830-4a34-8502-fab5201ddfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100,1000) \n",
    "# 100 observation 1000 features\n",
    "\n",
    "n_layers = 10 # the number of hidden layers is 10\n",
    "\n",
    "layer_sizes = [100]*n_layers # the number of nodes in each layers are 100\n",
    "\n",
    "# Forward propagation\n",
    "\n",
    "Hs = [X]\n",
    "Zs = [X]\n",
    "Ws = []\n",
    "\n",
    "# just traverse all the layer once with small random value initialization\n",
    "for i in np.arange(n_layers):\n",
    "    H = Hs[-1]\n",
    "\n",
    "    W = np.random.randn(layer_sizes[i],H.shape[0]) * 0.01\n",
    "    Z = np.dot(W,H) # linear connection \n",
    "    H = Z * (Z>0) # relu to make it non-linear\n",
    "    Hs.append(H)\n",
    "    Ws.append(W)\n",
    "    Zs.append(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca59c-9820-4d29-9c87-c995222b5c76",
   "metadata": {},
   "source": [
    "# the problem of initializing parameter with small random weight \n",
    "- activation means decays to zero\n",
    "- activation std decays to zero\n",
    "## partial derivative of loss with regard to W (parameter) \n",
    "\n",
    "- I wanna review \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fbfbe25-3b61-404b-a9fb-d49aa3380476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndLdH = 100 * np.random.randn(100,1000)\\nloss_grads = [dLdH]\\ngrads = []\\n\\nfor i in np.flip(np.arange(1,11), axis = 0):\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back prop\n",
    "\n",
    "# I wanna review this by writing \n",
    "\n",
    "\"\"\"\n",
    "dLdH = 100 * np.random.randn(100,1000)\n",
    "loss_grads = [dLdH]\n",
    "grads = []\n",
    "\n",
    "for i in np.flip(np.arange(1,11), axis = 0):\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ede119-00c4-4f20-9080-6babbadee3a9",
   "metadata": {},
   "source": [
    "# output activation is close to zero \n",
    "- gradients of loss with regard to parameter decay\n",
    "- the deeper the model goes, the more likely the gradient decays.\n",
    "\n",
    "# for smaller neural network, intialization with small random value is good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5b4b7-f536-4351-b4be-dee2d1c45475",
   "metadata": {},
   "source": [
    "# large random weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de5956ca-d6fe-4565-9d3d-c2fc90c5423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100,1000)\n",
    "n_layers = 10\n",
    "layer_sizes = [100]*n_layers\n",
    "\n",
    "Hs = [x]\n",
    "Zs = [x]\n",
    "Ws = []\n",
    "\n",
    "for i in np.arange(n_layers):\n",
    "    H = Hs[-1]\n",
    "\n",
    "    W = np.random.randn(layer_sizes[i],H.shape[0])*1\n",
    "    Z = np.dot(W,H)\n",
    "    H = Z * (Z>0)\n",
    "    Hs.append(H)\n",
    "    Zs.append(Z)\n",
    "    Ws.append(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2762a87-489a-465d-8d81-eb71adfa2a5e",
   "metadata": {},
   "source": [
    "# units explode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207d797-c342-4d7b-b716-91b96eee5704",
   "metadata": {},
   "source": [
    "# for other activation function like tanh()\n",
    "- small >> decay\n",
    "- large >>> I wanna review (2/3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a244dd2-93a6-48ba-9766-76d5cd57943b",
   "metadata": {},
   "source": [
    "# Xavier initialization\n",
    "\n",
    "## how to initiaize weight in i th layer\n",
    "\n",
    "- randomly pick weight from normal distribution\n",
    "  - mean = 0\n",
    "  - variance  = 2 / (# of input nodes + # of output nodes)\n",
    "- or\n",
    "- randomly pick weight from uniform distribution\n",
    "  - lower = - sqrt(6) / (nin + nout)\n",
    "  - higher = sqrt(6) / (nin + nout) \n",
    "\n",
    "## advantage\n",
    "\n",
    "- activations in each layer has the variant values.\n",
    "  - which prevent bias of activation\n",
    "- with sigmoid, Xavier initialization is good\n",
    "- fine with tanh\n",
    "\n",
    "## disadvantate \n",
    "\n",
    "- causes Relu to die\n",
    "  - because when input is negative, activation is zero\n",
    "  - if the half of the input is negative>> half is zero >> variance changes ==changes by half\n",
    "  -  so introduce another \n",
    "    -  normalizer in uniform(- sqrt(6) / (nin + nout), sqrt(6)/ (nin + nout) )\n",
    "      \n",
    "\n",
    "## assumption / you wanna check \n",
    "\n",
    "- work well when activation function is symmetric around zero\n",
    "  - sigmoid\n",
    "  - tanh\n",
    "\n",
    "- variance of the units accross all layers should be the same\n",
    "- backpropagated gradients accross all layers should be the same\n",
    "\n",
    "\n",
    "- input has equal variance across all dimensions\n",
    "  - each layer should have the same statistics ?? why\n",
    "\n",
    "- var(hi) == var(hj)\n",
    "- var(gradient of loss wrt hi) = var(gradient of loss wrt hj)\n",
    "  - hi is unit in i th layer\n",
    "  - hj is unit in j th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ec96850-9264-4074-a07f-d3397225d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100,1000)\n",
    "n_layers = 10\n",
    "layers_sizes = [100]*n_layers\n",
    "\n",
    "\n",
    "Hs = [x]\n",
    "Zs = [x]\n",
    "ws = []\n",
    "\n",
    "for i in np.arange(n_layers):\n",
    "    H = Hs[-1]\n",
    "\n",
    "    W = np.random.randn(layers_sizes[i],H.shape[0])* np.sqrt(2) / (np.sqrt(100 + 100) )\n",
    "    z = np.dot(W,H)\n",
    "    a = np.tanh(z)\n",
    "    Hs.append(a)\n",
    "    Ws.append(W)\n",
    "    Zs.append(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e04db-f5ca-4b4c-8417-e681f260f5a9",
   "metadata": {},
   "source": [
    "# He(MSRA) initialization with ReLU\n",
    "\n",
    "- usually used\n",
    "  - ReLU\n",
    "  - PReLU\n",
    "\n",
    "# advantage \n",
    "\n",
    "# disadvantage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2a83546-9b78-4429-87bf-4e6cc9e9ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100,1000)\n",
    "n_layers = 10\n",
    "layers_sizes = [100] * n_layers\n",
    "\n",
    "Hs = [x]\n",
    "Zs = [x]\n",
    "Ws = []\n",
    "\n",
    "\n",
    "for i in np.arange(n_layers):\n",
    "    H = Hs[-1]\n",
    "\n",
    "    W = np.random.randn(layers_sizes[i],H.shape[0])*np.sqrt(2) /np.sqrt(layers_sizes[i])\n",
    "    Z = np.dot(W,H)\n",
    "    a = Z * (Z>0)\n",
    "    Hs.append(a)\n",
    "    Ws.append(W)\n",
    "    Zs.append(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
