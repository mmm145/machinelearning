{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7fb118-3b16-49c6-99ff-4009cbcd072b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Batch normalization\n",
    "\n",
    "- not only normalize the first input\n",
    "- but also normalize the input of each layer\n",
    "  - use the mean and variance of mini batch.  \n",
    "- normalize the output per each batch\n",
    "  - mean = 0\n",
    "  - variance = 1\n",
    "  - steady learning\n",
    "  - speed up\n",
    " \n",
    "- usually used in CNN\n",
    "  - without batch normalization\n",
    "    - convolutional layer\n",
    "    - relu layer\n",
    "    - pooling layer\n",
    "  - with batch normalization\n",
    "    - convolutional layer\n",
    "    - batch normalization layer\n",
    "    - relu layer\n",
    "    - pooling layer\n",
    "\n",
    "# the main reason why you wanna use batch normalizatoin\n",
    "\n",
    "- sometimes, the feature distribution of train and test can different so much.\n",
    "  - covariate shift.\n",
    "  - when updating the parameter, which would influence the other variable as well. \n",
    "- especially, for neural network,\n",
    "-  deep layers\n",
    "- covariate shift happens for each mini batch\n",
    "  - internal covariate shift.\n",
    "\n",
    " \n",
    "# why it is batch normalization instead of all data\n",
    "\n",
    "- not deep NN\n",
    "  - normalize input feature once can work\n",
    "- NN\n",
    "  - the input for each layer will change after parameter update\n",
    "  - it is time consuming to try to normalize every time.\n",
    "  - batch normalization smoothen the loss curve.\n",
    "\n",
    "# advantage \n",
    "\n",
    "- learning stability\n",
    "  - in NN, it would be better that if the input data is stable.\n",
    "- learning got faster\n",
    "- regularization\n",
    "  - you might not have to use drop out but you can use it.\n",
    "  - when the neural network is too large\n",
    "\n",
    "- you can use bigger learning rate without gradient decay/explosion\n",
    "- the effect of regularization\n",
    "  - ex)\n",
    "    - the model does not need L2 regularization\n",
    "    - the model does not need drop out.\n",
    "- make the learning faster\n",
    "  - drop out prevents overfitting from happening\n",
    "  - but\n",
    "  - drop out slows the learning prcess.\n",
    " \n",
    "- the effect of initialization goes down. \n",
    "\n",
    "# disadvantage\n",
    "\n",
    "- increase parameters to learn.\n",
    "- more complicated for backpropagation\n",
    "\n",
    "# how\n",
    "\n",
    "- Normalize the unit activation\n",
    "  - $\\hat{x_i} = \\frac{(x_i - µ_i)}{\\sqrt{\\sigma_i^2 + \\epsilon}} $\n",
    "  - µ is sample mean in\n",
    "  - and then\n",
    "  - you do scaling and shifting\n",
    "  - $bn{x_i} = r_i \\hat{x_i} + b_i$\n",
    "- usually after affine before activation\n",
    "  - affine\n",
    "  - bn\n",
    "  - relu\n",
    " \n",
    "  -  when using bn layer, you do not need bias in affine because shifting after normalization works as bias\n",
    " \n",
    "**back propagation**\n",
    "\n",
    "- write the computational graph\n",
    "  - input\n",
    "      - x\n",
    "      - mean\n",
    "      - variance\n",
    "      - epsilon\n",
    "      - gamma\n",
    "      - beta\n",
    "  - operators\n",
    "    -       +\n",
    "    -       -\n",
    "    -       *\n",
    "    -  inv\n",
    "    -  sqrt\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf29f6-4ea1-4ab2-a493-44f191991de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
