{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955e245a-ea32-4add-902a-fb7fa7c7319c",
   "metadata": {},
   "source": [
    "# regularization \n",
    "\n",
    "\n",
    "## stopping early.\n",
    "\n",
    "- stop the training when the validation loss does not improve anymore.\n",
    "  - hyper parameter\n",
    "      - the number of validation loss does not improve\n",
    "- need cache for validation loss.\n",
    "- validation loss can be calculated as parallel to the training loss.\n",
    "- model or class does not have to be changed.\n",
    "\n",
    "## Dataset augmentation\n",
    "\n",
    "-  dataset augumentation.\n",
    "  - eg)\n",
    "    - image\n",
    "      - upside down\n",
    "      - masking\n",
    "      - colorless\n",
    "      - etc... \n",
    "- inject noise into the network\n",
    "-  label smoothing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## add norm penalty to the loss function\n",
    "\n",
    "loss_hat = loss + penalty(w)   \n",
    "\n",
    "- lasso\n",
    "  - / w /\n",
    "  - work as variable selection\n",
    "    - but if multiple variable has correlation, lasso try to reduce all of them\n",
    "- ridge\n",
    "  - transpose(w) * w / 2\n",
    "  - make parameter close to zero.\n",
    "  -  when you already know the parameter will get close to a certain parameter\n",
    "    - // w - b //^2\n",
    "    - or\n",
    "    - // w(1) - w(2) //^2\n",
    "- elastic net\n",
    "  - the combination of ridge and lasso\n",
    "  -  work as variable selection\n",
    "    - if multiple variable has the correlation, pick some of them to reduce.\n",
    "## multi task learning\n",
    "\n",
    "- when multiple tasks has interactions between each others.\n",
    "  - if one task lacks of something\n",
    "  - but\n",
    "  - the other has the information\n",
    "  - and also\n",
    "  - there is a interaction between them\n",
    "  - the model can improve based on the task that has the information \n",
    "\n",
    "- traditionally, it is common to use each model for each task.\n",
    "- use\n",
    "  - common part\n",
    "  - relation\n",
    "  - correlation\n",
    "  - between multiple tasks\n",
    "    - so that the model can deal with multiple tasks at the same time.\n",
    "\n",
    "**benefit**\n",
    "\n",
    "- the model can share the multiple tasks from the multiple tasks.\n",
    "\n",
    "- reuse the data and the resource.\n",
    "- the interaction between multi tasks.\n",
    "  - the improvement for both task`s performance\n",
    "- reduce the number of labels.\n",
    "\n",
    "**choose and construct tasks**\n",
    "\n",
    "- difficulity of tasks.\n",
    "  - the difficulity of task should be similar\n",
    "  - if some of them are easy and some of them are difficult, then the model will focus on easier one more. \n",
    "- order of tasks\n",
    "- collecting data and preprocess.\n",
    "\n",
    "## transfer training\n",
    "\n",
    "- use model that is already trained with some train data\n",
    "  - to learn another train data with additional information.\n",
    "- the size of train data is small\n",
    "- and\n",
    "- the task is similar.\n",
    "\n",
    "- when the size of train data is large\n",
    "  - you can tune more new layer by using transfer learning.\n",
    " \n",
    "## Esemble learninig\n",
    "\n",
    "- train weaker models\n",
    "- and\n",
    "- average/aggregate the reuslt\n",
    "  - classification - mode\n",
    "  - regression - average.\n",
    " \n",
    "- if models are independent\n",
    "  - they tend not to make the same error\n",
    "  -  the model can learn some variety of error.\n",
    "  -  \n",
    "\n",
    "- tree\n",
    "  - random forest tree\n",
    "\n",
    "### baggin (one methods for esemble learning) \n",
    "\n",
    "- bootstrap\n",
    "- and\n",
    "- aggregating\n",
    "1. bootstrappin the k data set\n",
    "2. train k different models for k data set.\n",
    "3. aggregating k results from the k different models.\n",
    "\n",
    "- for neural network\n",
    "  - just changing the hyper parameter can be considered to be different models.\n",
    "  - but\n",
    "  - it takes a lot of time if you want to implement the ensemble learning for neural network\n",
    "    -  you can get each snapshot of local minimum and average them\n",
    "   \n",
    "## drop out\n",
    "\n",
    "- for neural network\n",
    "- like baggin within signle neural network\n",
    "- by drop some of the units in some layers.\n",
    "\n",
    "**how**\n",
    "\n",
    "- arrange the mask vector\n",
    "  - each element is from bernoulli distribution\n",
    "    -  1 with p=p or 0 with p=1-p\n",
    "    -  usually\n",
    "      - p = 0.8 for input layer\n",
    "      - p = 0.5 for hidden layer.\n",
    "   \n",
    "**training**\n",
    "\n",
    "- after you get the z(i)\n",
    "- you create the mask(i) whose shape is the same as z(i)\n",
    "- multiply mask(i) to z(i)\n",
    "\n",
    "**testing**\n",
    "\n",
    "- after many times of learning\n",
    "- the contribution of w(i)h(i) is just p * w(i) h(i)\n",
    "-  so for testing\n",
    "-  you just have to multiply the affine with p\n",
    "-  or\n",
    "-  multiple the activate(affine()) with p\n",
    "\n",
    "**inverted dropout**\n",
    "\n",
    "- maintain the expectation of output as same as without dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23621df1-e2dc-4498-b560-7023fc1f0edb",
   "metadata": {},
   "source": [
    "# Multtask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9668c6-9e4c-4ea5-a80a-aac755fe2e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class Multitask_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.digit_fc = nn.Linear(32, 10) \n",
    "        self.parity_fc = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        digit_logit = self.digit_fc(x)\n",
    "        parity_logit = self.parity_fc(x)\n",
    "        return digit_logit, parity_logit\n",
    "\n",
    "model = Multitask_Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        img, label = batch\n",
    "        parity_label = label % 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        digit_logit, parity_logit = model(img)\n",
    "        \n",
    "        digit_loss = criterion(digit_logit, label) \n",
    "        parity_loss = criterion(parity_logit, parity_label)\n",
    "        \n",
    "        loss = digit_loss + parity_loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f'epoch: {epoch}, loss: {avg_loss}')\n",
    "\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05df137a-932d-401a-9d92-71679356e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ab5fb-a00b-4fa2-990a-76fcdbbf0ed9",
   "metadata": {},
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d9e298-d3d5-4fe7-90da-1e5ece0cdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "relu = lambda x: x * (x>0) \n",
    "\n",
    "def forward(x):\n",
    "    a1 = np.dot(w1,x) + b1\n",
    "    z1 = relu(a1)\n",
    "    # get the usual output for layer 1 withtout dropout\n",
    "    m1 = np.random.rand(*z1.shape)<p\n",
    "    # get the mask whose shape is the same as z1\n",
    "    z1 *= m1\n",
    "    # multiply the \n",
    "\n",
    "\n",
    "    a2 = np.dot(w2, z1)+b2\n",
    "    z2 = relu(a2)\n",
    "    m2 = np.random.rand(*z2.shape) <p\n",
    "    z2 *=m2\n",
    "\n",
    "    out = np.dot(w3,z2)+b3\n",
    "    return out\n",
    "\n",
    "\n",
    "def test(X):\n",
    "    \n",
    "    a1 = np.dot(w1,x) + b1\n",
    "    z1 = relu(a1) * p\n",
    "    a2 = np.dot(w2, z1)+b2\n",
    "    z2 = relu(a2) * p\n",
    "    out = np.dot(w3,z2)+b3\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb22d8-d7cb-4521-bf3a-db2847ca7c1b",
   "metadata": {},
   "source": [
    "# inverted drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c73207a-9763-424a-8c58-15f9af2d6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "relu = lambda x: x*(x>0)\n",
    "\n",
    "def forward(x):\n",
    "    a1 = np.dot(w1,x) + b1\n",
    "    z1 = relu(a1)\n",
    "    m1 = (np.random.rand(*z1.shape) <p )/ p\n",
    "    z1 *= m1\n",
    "\n",
    "    a2 = np.dot(w2,z1) + b2\n",
    "    z2 = relu(a2)\n",
    "    m2 = (np.random.rand(*z2.shape) <p ) / p\n",
    "    z2 *= m2\n",
    "\n",
    "    out = np.dot(w3,z2) + b3\n",
    "\n",
    "def test(X):\n",
    "    a1 = np.dot(w1,x) + b1\n",
    "    z1 = relu(a1)\n",
    "    a2 = np.dot(w2,z1) + b2\n",
    "    z2 = relu(a2)\n",
    "    out = np.dot(w3, z2) + b3\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
