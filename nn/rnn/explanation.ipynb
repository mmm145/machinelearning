{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f3a255-d258-4331-8c65-9c3dca011046",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "- to deal sequential data\n",
    "  - text\n",
    "  - audio\n",
    "- update information and use the information to get information\n",
    "  - memory\n",
    " \n",
    "- natural language processing.\n",
    "\n",
    "\n",
    "## motivation (Speech recognition)\n",
    "\n",
    "we want to transform the wave data of speech into text.\n",
    "\n",
    "**problem**\n",
    "\n",
    "- Input can be of variable size\n",
    "\n",
    "standard neural networks can only handle data of a fixed input size. \n",
    "\n",
    "- we need neural network that can handle the sequential data.\n",
    "\n",
    "\n",
    "## Inductive bias (pre assumption of data.)\n",
    "\n",
    "**reccurent inductive bias**\n",
    "\n",
    "- the seqential processing of the input\n",
    "  - input has order\n",
    "  - output has order\n",
    "  - hidden information has order\n",
    "  - use previous output, hidden information and input at the point to get next output and hidden information\n",
    "- no direct acess to the past tokens\n",
    "  - hidden memory\n",
    "  - accessible from next token\n",
    "- recursion\n",
    "  - the model apply the same function to the different input several times.\n",
    "\n",
    "## mathematical formula \n",
    "\n",
    "**input**\n",
    "\n",
    "- $x_{t}(0,1,2,...,t)$\n",
    "- or \n",
    "- $h_{-1}$\n",
    "\n",
    "**recusive output**\n",
    "- $h_{t} =  \\sigma(\\boldsymbol{W}^{T}_{h}[h_{t-1},x_t])$\n",
    "- $y_t = \\sigma(\\boldsymbol{W}^{T}_{y}h_t)$\n",
    "\n",
    "**output**\n",
    "\n",
    "- $y_{t}$ all or just the last one\n",
    "- \n",
    "- $h(t)$ and $y(t)$\n",
    "\n",
    "## BackPropagation Through Time (BPTT) \n",
    "\n",
    "- usual back propagation works\n",
    "\n",
    "\n",
    "**difficulity**\n",
    "\n",
    "- long horizontal propagation.\n",
    "  - vanishing / exploding gradients.\n",
    "  - large memory/ computational footprint.\n",
    " \n",
    "**solution**\n",
    "\n",
    "- naive\n",
    "  - skip connection between each rnn\n",
    "\n",
    "- memory network\n",
    "  - long short-term memory (LSTM)\n",
    "  - \n",
    "\n",
    "\n",
    "## Memory networks\n",
    "\n",
    "- LSTM\n",
    "- GRU\n",
    "- NTM\n",
    "- MemNN\n",
    "- DNC\n",
    "\n",
    "## Bi-directional RNNs\n",
    "\n",
    "- simple RNN\n",
    "  - uses information beforehand\n",
    " \n",
    "- bi-directional Rnn\n",
    "  - uses future information also as input.\n",
    " \n",
    "### seq2vec\n",
    "\n",
    "### vec2seq\n",
    "\n",
    "### seq2seq \n",
    "\n",
    "\n",
    "\n",
    "## Auto-regressive generative modeling.\n",
    "\n",
    "- RNN\n",
    "  - output becomes next input\n",
    "\n",
    "## hyper parameters\n",
    "\n",
    "**method**\n",
    "- grid search\n",
    "- random search\n",
    "- baysian optimizatoin\n",
    "- and\n",
    "- cross validation \n",
    "\n",
    "\n",
    "**hyper parameters**\n",
    "\n",
    "- activation function\n",
    "  - sigmoid\n",
    "    - 1 / ( 1 + exp( -x ) )\n",
    "  - tanh\n",
    "    - 2sigmoid(2x)-1\n",
    "    - ( exp( x ) - exp( -x ) ) / ( exp( x ) + exp( -x ) )\n",
    " \n",
    "  -  when x -> +inf, x-> -inf >> gradient will be zero.\n",
    "    - Vanishing gradient\n",
    " \n",
    "  - Relu\n",
    "    - max(0,x)\n",
    "    - when input is negative > dead units.\n",
    "    - in practice, this does not seem to be a problem. \n",
    "  - Leaky Relu / PRelu\n",
    "    - PReLU(x) =\n",
    "      - a*x x<=0\n",
    "      - x   x >0 \n",
    "  - ELU\n",
    "    - a( exp( x ) - 1 ) x<=0\n",
    "    - x  x>0 \n",
    "  - SiLU\n",
    "    - x / ( 1 + exp( -x ) ) \n",
    "  - Softplus\n",
    "    - log( 1 + exp( x ) )\n",
    "   \n",
    "- batch size\n",
    "  - performance\n",
    "  - training time\n",
    "  - the larger, the more accurate estimate of the gradient\n",
    "  - the smaller, the faster convergence and the effect of convergence.\n",
    " \n",
    "- Regularization\n",
    "  - batch normalization\n",
    "    - optimization tricks\n",
    "    - normalize the output of a previous activation layer\n",
    "      - batch mean\n",
    "      - batch standard deviation. \n",
    "  - SGD\n",
    "  - drop out\n",
    "  - early stopping\n",
    "  - weight penality\n",
    "    - lasso\n",
    "    - ridge\n",
    "    - elastic\n",
    "\n",
    "\n",
    "   \n",
    "## optimization tricks\n",
    "\n",
    "**optimization methods**\n",
    "  - GD\n",
    "  - SGD\n",
    "  - mBGD \n",
    "  - Adam\n",
    "    - adaptive learning rate properties. \n",
    "  - Rmsprop\n",
    "\n",
    "**optimization tricks**\n",
    "\n",
    "- batch normalization\n",
    "- gradient clipping\n",
    "  - when gradient is too large.\n",
    "  - \n",
    "- learning rate scheduling\n",
    "  - adjusting learning rate during training. \n",
    "- early stopping\n",
    "  - stop optimization when validation error starts increasing. \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
