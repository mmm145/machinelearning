{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f407289e-9f84-4ffc-81ef-41ea30d6a791",
   "metadata": {},
   "source": [
    "# representation learniing \n",
    "\n",
    "- wanna find the encoder that capture the good information or not lose the information of original data.\n",
    "  - information should be\n",
    "    - compact - capture essential information in fewer dimensions\n",
    "      - like maximize the variance of priceple component in PCA\n",
    "    - disentangled -  separate different factors of variation\n",
    "      - like each component should be orthogonal. \n",
    "    - transferable -  useful for multiple downstream tasks.\n",
    "      - the pattern shouold be applied to another data\n",
    "     \n",
    "## what is the difference between PCA\n",
    "\n",
    "- pca is only for linear feature\n",
    "  - you can use kernel trick in theory. \n",
    "- pca can not handle the large amount of data\n",
    "  - << SVD is time consuming\n",
    "- pca can not handl missing data\n",
    "\n",
    "# benefits\n",
    "\n",
    "- dimension reduction\n",
    "- do not need that much labeled data. >>> cost effective.\n",
    "- domain - specific \"common sense\"\n",
    "- learn meaningful representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e0c43-6857-4a4c-a0e0-6eb8366a3f86",
   "metadata": {},
   "source": [
    "- p(x) = $\\sum^{K}_{k=1}π_kN(x | µ_k, ∑_k)$\n",
    "\n",
    "# Autoencoders \n",
    "\n",
    "- encoder(x)\n",
    "- decode(x)\n",
    "\n",
    "- dimension reduction\n",
    "- clearning data\n",
    "- learning feature\n",
    "\n",
    "# model\n",
    "# assumption\n",
    "\n",
    "- you can not do encode(x) = x\n",
    "  - not compact\n",
    "  - not separate factors of variation\n",
    "  - not capture the pattern / structure of data\n",
    " \n",
    "- so\n",
    "  - dimension of encode(x) < dimension of x\n",
    "  - adding regularization\n",
    "  - introducing noise.\n",
    "    - denoising autoencoders. \n",
    "\n",
    "# objective function / loss\n",
    "\n",
    "- parameter = argmin $\\sum_{i=1}^{N} L2-norm(x_{i} - decode(encode(x_{i}))^2$\n",
    "\n",
    "\n",
    "# denoising autoencoders\n",
    "\n",
    "- so that encoder can caputer the pattern of original data\n",
    "- adding the noise and make data corrupted.\n",
    "  - $\\hat{x_i} = corrupt(x)$\n",
    "    - how to denoising the autoencoder?\n",
    "  - parameter = argmin $\\sum_{i=1}^{N} L2-norm(\\hat{x_i} - decode(encode(\\hat{x_i}))^2$\n",
    "- and recover clean data\n",
    "- << the encode capture only the patter and structure of input data\n",
    "\n",
    "## types of corruption\n",
    "\n",
    "- Gaussian Noise\n",
    "  -  $\\hat{x_i} = N(x, \\sigma^2*I)$\n",
    "  -  robustness to small perturbations\n",
    "    - data pertubations means denosing the data. \n",
    "- Masking Noise\n",
    "  - $\\hat{x_i}$ = 0 with (p=v) or $x_{i}$ with (p = 1-v)\n",
    "  - feature completion and correlation \n",
    "- Salt - and - Pepper noise (SP):\n",
    "  - $\\hat{x_i}$ = 0 or 1 with (p=v) or $x_{i}$ with (p = 1-v)\n",
    "  - handling outliers and extreme values.\n",
    " \n",
    "## types of autoencoder \n",
    "\n",
    "- Masked AutoEncoder\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ed640-a522-45d6-bfcb-1b0415aa45bf",
   "metadata": {},
   "source": [
    "# Masked AutoEncoder (MAE)\n",
    "\n",
    "- mask the part of input data\n",
    "  - high mask ratio 75% to 80%\n",
    "  - encode visible patches with VIT\n",
    "  - ????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
