{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99b5b30-69bf-467c-a164-f3ed1b319065",
   "metadata": {},
   "source": [
    "# bayse theorem \n",
    "\n",
    "related to conditional probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c14ae-447e-480a-9a45-fbdec7561011",
   "metadata": {},
   "source": [
    "# MAP ( maximum a posteriori )\n",
    "\n",
    "P(Y / X ) = P(X / Y)P(Y) / P(X) \n",
    "\n",
    "is parallel to P(X/Y)P(Y) = P(X1, X2, ... Xn / Y) P(Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b37f55-58ee-4d34-a6a0-7bae91c130d0",
   "metadata": {},
   "source": [
    "# naive bayse\n",
    "\n",
    "sometimes used for baseline of evaluation.\n",
    "\n",
    "**type of nb**\n",
    "\n",
    "you wanna choose each from the distribution of Y\n",
    "\n",
    "- naive bayse\n",
    "- gaussian\n",
    "    - regression\n",
    "    - classification \n",
    "- multinomial\n",
    "    - doing categorical trial n times\n",
    "    - vector p = (p1,p2,p3,...pk)\n",
    "- complement\n",
    "- bernouill\n",
    "  - discrete for binary random variable(0/1)\n",
    "  - parameters p - proabability of success\n",
    "  - PMF p(y) = pow(p,y)*pow(1-u,1-y) y = 0 or y = 1\n",
    "  - Log-likelihood p(y) = y logp + (1-y)log(1-p)\n",
    "  - MLE - p = mean(y)\n",
    "- categorical\n",
    "  - parameters - vector p = (p1,p2,p3,,,,pk) pi is the probability for i-th class sum(ps) = 1\n",
    "  - PMF - p(y=k) = pk\n",
    "  - Log - likelihood p(y=k) = log(pk)\n",
    "  - MLE - pk = (# of k) / all #\n",
    "- binomial\n",
    "  - doing bernoulli n times\n",
    "  - parameters  n (number of trials ) / p - probability of success\n",
    "  - PMF p(y) = nCy pow(p,y)*pow(1-p,n-y)\n",
    "  - Log-likelihood logP(y) = log [n C y] + y logp + (n-y)log(1-u)\n",
    " \n",
    "\n",
    "## what type of likelihood function you wanna use\n",
    "\n",
    "depends on the distribution of explanatory variables, features.\n",
    "\n",
    "\n",
    "## you have to think before \n",
    "\n",
    "### assumption \n",
    "\n",
    "\n",
    "- bayes theorem.\n",
    "\n",
    "- all attributes are conditonaly independent >>>> reduce the parameters to 2n \n",
    "- if all features are not conditionally independent large number of data enable us to use NB\n",
    "\n",
    "### preprocess\n",
    "\n",
    "\n",
    "\n",
    "## pro \n",
    "- scalability - good for large data (in python, you can use partial fit ) \n",
    "- speed\n",
    "- interpretability\n",
    "## con \n",
    "\n",
    "\n",
    "\n",
    "## smoothing \n",
    "\n",
    "if p( X = a / Y = y) = 0 \n",
    "\n",
    "-> when testing, p(X = x1 / Y = y) * p(X = x2 / Y = y)..p(X = a / Y = y)*....p(X = xn / Y = y) = 0 \n",
    "\n",
    "- add 1\n",
    "  - p (X = xi / Y = y ) = (# D {Xi = xi and Y = y } +  1) / (# D{ Y = y } + abs(Xi))\n",
    "- add alpha\n",
    "  - p(X = xi / Y = y) = (# D {Xi = xi and Y = y } + alpha) / (# D{Y = y}+ alpha * abs(Xi))\n",
    "\n",
    "## numerical stability \n",
    "\n",
    "\n",
    "**when multiplying many amount of probability**\n",
    "\n",
    "- think aobut flipping coin 2000 times\n",
    "  - (0.5) ** 2000 close to 0\n",
    "\n",
    "- in some range, multiplying many amount of probability leads to 0\n",
    "- so, you wanna sum up the log. \n",
    "\n",
    "**-log{probability}**\n",
    "\n",
    "- -log{probability} happens when calculating the log of baysian theorem.\n",
    "- leading to overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e770fd-52b8-457d-ad08-9de99f7d07fc",
   "metadata": {},
   "source": [
    "# Categorical Naive bayse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704760d-d538-4357-996c-d2050f3e68ed",
   "metadata": {},
   "source": [
    "# bernouill naive bayse\n",
    "\n",
    "bernoulli distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2ff60-d561-490e-9cf4-70c18fd5476a",
   "metadata": {},
   "source": [
    "# multinomial naive bayse  \n",
    "\n",
    "## each word comes from categorical distribution.\n",
    "\n",
    "## the total comes from multinomial distribution.\n",
    "\n",
    "## when features are multinomial \n",
    "\n",
    "- text classification is the best example.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
