{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c584d448-be8b-4ad2-8f13-e98fee61fde8",
   "metadata": {},
   "source": [
    "# perceptron\n",
    "\n",
    "**perceptron is**\n",
    "\n",
    "- perceptron is used for binary linear classifier.\n",
    "- \n",
    "\n",
    "\n",
    "**Given**\n",
    "- dataset $\\{(\\boldsymbol{x}^{(1)}, t^{(1)}), ..., (\\boldsymbol{x}^{(N)}, t^{(N)})\\}$\n",
    "\n",
    "\n",
    "  $\\boldsymbol{x}^{(i)} \\in \\boldsymbol{R}^d$\n",
    "\n",
    "  =\n",
    "\n",
    "  $\\boldsymbol{x}^{(i)} = \\{x^{(1)}, x^{(2)}, ...., x^{(d)}\\}$\n",
    "\n",
    "  for all i, ( $1 \\leq i \\leq N)$\n",
    "-  $t^{(i)} \\in \\{ 0, 1\\}$\n",
    "\n",
    "\n",
    "**data preprocess**\n",
    "\n",
    "- scaling ( normalizing or standardizing) because perceptron is sensitive to scale of each features. \n",
    "\n",
    "**hyper parameter**\n",
    "\n",
    "\n",
    "-  threshhold for the prediction ( $y^{(i)} = 0$ if $\\boldsymbol{w}・\\boldsymbol{x} + w_0 < c$ or $y^{(i)} = 1$ if $\\boldsymbol{w}・\\boldsymbol{x} + w_0 \\geq c$)\n",
    "-  optimization algorithm (GD/SGD/mini-batch SGC etc) \n",
    "- learning rate ( the bigger, the faster but more likely to diverge / the smaller, the more preicise and more likely to converge but slower)\n",
    "- number of epochs \n",
    "- initial value of parameters (weights/bias)\n",
    "- activation function ( usual perceptron uses step functon as activation function, but you might wanna try others like sigmoid, Relu..)\n",
    "- whether shuffle the training data before each epochs. (shuffling is good for dealing with overfitting)\n",
    "- stopping criteria (usually tolerance)\n",
    "- early stopping or not.\n",
    "- whether you apply regularization\n",
    "\n",
    "**drawbacks**\n",
    "\n",
    "- when the data set is not linearily separable, perceptron algorithm will never converge.\n",
    "- result is not probabilstic. ( while logstic regression use sigmoid activation function to get probabilstic result)\n",
    "- it is hard to generalize to the case when class is greater than 2 (you can generalize with one vs all or one vs one tho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7501884-674b-4f88-b783-412b2397304d",
   "metadata": {},
   "source": [
    "# training\n",
    "\n",
    "optimization is done by the most basic method, gradient descent.\n",
    "\n",
    "1. **initialize parameters**\n",
    "2. **get activation**\n",
    "\n",
    "   - $a = \\boldsymbol{w}^{T}・\\boldsymbol{x} + w_0$\n",
    "3. **get prediction by plugging activation into activation function**\n",
    "\n",
    "   - $f(a) = 1$ if $a \\geq 0  $ or $=0$ if $a < 0 $\n",
    "   -  $= \\hat{y}$\n",
    "4. **get weigh update from perceptron learning algorithm**\n",
    "\n",
    "    - $\\Delta{\\boldsymbol{w}} = \\eta \\boldsymbol{X}^T(\\hat{y}-y)$\n",
    "    - $\\Delta{w_0} = \\eta (\\hat{y} - y)$\n",
    "5. **update**\n",
    "\n",
    "  - $\\boldsymbol{w}^{k+1} = \\boldsymbol{w}^{k} + \\Delta{\\boldsymbol{w}}$\n",
    "  - $w_{0}^{k+1} = w_{0}^{k} + \\Delta{w_0}$\n",
    "\n",
    "(if you put column vector whose element is only 1 and concate the vector to $\\boldsymbol{X}$, then you do not have to separate $\\boldsymbol{w}$ and $w_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18686fd-b223-4abe-885b-c4a6c3995dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
