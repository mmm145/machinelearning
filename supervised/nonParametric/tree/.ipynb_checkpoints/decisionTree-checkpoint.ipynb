{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3c725d-db1d-49ee-8e93-3f1298b0e8d4",
   "metadata": {},
   "source": [
    "# decision tree\n",
    "\n",
    "\n",
    "\n",
    "## Assumption \n",
    "\n",
    "- data point has the same class label when it assingend to the same leaf\n",
    "- so use how pure the leaf as loss function\n",
    "\n",
    "## advanteage \n",
    "\n",
    "- you do not have to load the full training set\n",
    "\n",
    "## disadvantage\n",
    "\n",
    "\n",
    "## type of decision tree\n",
    "\n",
    "- ID3(Iterative Dichotomizer 3)\n",
    "- C4.5(successor of ID3)\n",
    "- CART(Classification And Regression Tree)\n",
    "\n",
    "## how \n",
    "\n",
    "### decesion tree devide the feature space.\n",
    "\n",
    "- each rectangle is equivalent to category or probability \n",
    "\n",
    "### learning \n",
    "\n",
    "- NP- complete problem\n",
    "  - you can not take derivative, so you can not do gradient based approach.\n",
    "\n",
    "\n",
    "## Recursive construction \n",
    "\n",
    "tree = (root, subtree) \n",
    "\n",
    "- G(x): full-tree hypothesis\n",
    "- b(x): branching criteria\n",
    "- Gc(x): sub-tree hypothesis at the c-th branch\n",
    "\n",
    "G(x) = sum of all branch of ( b(x) == c  ) * Gc(x)\n",
    "\n",
    "## basic decision tree algorithm with recursive algorithm\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "## hyper parameter \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
