{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3c725d-db1d-49ee-8e93-3f1298b0e8d4",
   "metadata": {},
   "source": [
    "# decision tree\n",
    "\n",
    "- non-parametric\n",
    "- supervised\n",
    "  - classification\n",
    "  - regression\n",
    "\n",
    "\n",
    "\n",
    "## 1.Assumption \n",
    "\n",
    "- data point has the same class label when it assingend to the same leaf\n",
    "- so use how pure the leaf as loss function\n",
    "\n",
    "## 2.1 advanteage \n",
    "\n",
    "- easy to understand\n",
    "- require little data preparation\n",
    "- computationally efficient\n",
    "- handle both numerical and categorical\n",
    " \n",
    "## 2.2 disadvantage\n",
    "\n",
    "-  NP-hard\n",
    "-  resort to greedy approach \n",
    "\n",
    "\n",
    "## 3.type of decision tree\n",
    "\n",
    "- ID3(Iterative Dichotomizer 3)\n",
    "  - maximize information gain\n",
    "- C4.5(successor of ID3)\n",
    "  - use gain ratio to reduce bias\n",
    "- CART(Classification And Regression Tree)\n",
    "  - uses gini impurity for classification, leading to binary splits.\n",
    "\n",
    "## 4. how \n",
    "\n",
    "### 4.1 decesion tree devide the feature space.\n",
    "\n",
    "- each rectangle is equivalent to category or probability \n",
    "\n",
    "### 4.2 learning \n",
    "\n",
    "- NP- complete problem\n",
    "  - you can not take derivative, so you can not do gradient based approach.\n",
    "\n",
    "\n",
    "## 4.3 Recursive construction \n",
    "\n",
    "tree = (root, subtree) \n",
    "\n",
    "- G(x): full-tree hypothesis\n",
    "- b(x): branching criteria\n",
    "- Gc(x): sub-tree hypothesis at the c-th branch\n",
    "\n",
    "G(x) = sum of all branch of ( b(x) == c  ) * Gc(x)\n",
    "\n",
    "## 4.4 basic decision tree algorithm with recursive algorithm\n",
    "\n",
    "- number of branches - C\n",
    "- criteria for splititng node - b(x)\n",
    "- criteria for terminating\n",
    "- base hypothesis to return when reaching a leaf.\n",
    "\n",
    "\n",
    "def basicdecisiontree(data):\n",
    "\n",
    "    if termination criteria:\n",
    "    \n",
    "        return base hypothesis\n",
    "    else:\n",
    "        learn spliting criteria (b(x))\n",
    "        split data to C parts Dc = {(Xi, yi): b(Xi) = c}\n",
    "        build subtree: Gc(X) = basicdecistiontree(Dc)\n",
    "        G(X) = sum of all splitted (b(X) = c) * Gc(X)\n",
    "\n",
    "## 5. metrics for measuring the impurity.\n",
    "\n",
    "### 5.1 uncertain measures\n",
    "\n",
    "- gaining information >> uncertainty\n",
    "- uncertainty of node == distribution of class in node.\n",
    "\n",
    "**key word**\n",
    "\n",
    "- entropy\n",
    "- conditional entropy\n",
    "- information gain\n",
    "  - standardized information gain = gain ratio\n",
    "- mutual information\n",
    "- Kullback - Leibler divergence \n",
    "- gain ratio\n",
    "  - standardized information gain = gain ratio\n",
    "- gini index \n",
    "\n",
    "**engropy**\n",
    "\n",
    "- H(x) = - sum through all class  (p(X = class i ) * log p(X = class i) )\n",
    "  - base of log = 2 or 10\n",
    "  - base of log = 2 >> unit of entropy is bit.\n",
    " \n",
    "- this shows uncertainty\n",
    "  - the larger, the more uncertain the classification in node (ex) all classes in node uniformly)\n",
    "  - the smaller, the more pure the classification in node (ex) one class of all is much a lot than others.)\n",
    "\n",
    "\n",
    "#### 5.1.1 how to calculate entropy for each node \n",
    "\n",
    "- plot histgram to estimate the probability.\n",
    "- or just get probability for each class from # of class i / # of all\n",
    "- then calculate the entropy for each node\n",
    "\n",
    "#### 5.1.2 how to calculate entropy for spliting criteria \n",
    "\n",
    "- weigted average for each entropy\n",
    "- sum of all node ( (entropy i ) * (# all data in i node regardless of class / # all data before spliting regardless of class)\n",
    "\n",
    "\n",
    "### 5.2 Measure of uncertainty of a split.  == what I wrote in 5.1.2\n",
    "\n",
    "- conditional entropy\n",
    "- when split the data based on feature a \n",
    "- H(X / A )  = sum of( p(A = a)*H(X / A = a) ) \n",
    "- = sum of ( p(A = a) * (- sum of all class (p(X/A=a) * log p(X / A=a ) ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5.3 information gain (ID3 algorithm)\n",
    "\n",
    "- Information Gain (IG) = IG(X,A) = H(X) - H(X / A)\n",
    "- how many information you can get by spliting based on A\n",
    "- prior entropy before spliting - conditional entropy based on A\n",
    "- entropy\n",
    "  - the larger, the more uncertain\n",
    "  - the smaller, the more certain\n",
    "- information gain\n",
    "  - if H(x) is large and H(X/A) is small >>> feature A is good splitting criteria.\n",
    " \n",
    "#### 5.3.1 Mutual information \n",
    "\n",
    "- this metric also can be used for feature selection\n",
    "\n",
    "- expected value of the information gain\n",
    "  - Mutual Information\n",
    " \n",
    "### 5.4 Distance between worst case and best candidate P \n",
    "\n",
    "- Kullback-Leibler divergence\n",
    "  - distance between two probability distribution p(x) and q(x)\n",
    "  - $\\ D_{KL}(p // q) = \\sum_{x \\in{X}} p(x)log\\frac{p(x)}{q(x)}$\n",
    "    - D(p // q) ≠ D(q // p)\n",
    "    - p - true distribution\n",
    "    - q - approximation of p\n",
    "\n",
    "#### 5.4.1 Maximize the KL distance == Minimize the entropy (because in this case, we measure distance between the worst case)\n",
    "\n",
    "- $D_{KL}(p // q) = \\sum_{x \\in{X}} p(x)log\\frac{p(x)}{q(x)}$\n",
    "  - = $\\sum_{k=1}^{C} p(k)log\\frac{p(k)}{q(k)}$ wiht $q(k) = \\frac{1}{C}$\n",
    "    - q(k) is the worst case\n",
    "  - expand and with knowledge s.t $\\sum_{k=1}^{C} p(k) = 1$\n",
    "  - =$\\sum_{k=1}^{C} p(k) log(p(k)) + logC$\n",
    "\n",
    "- $max_{p} D_{KL}(p // q ) $\n",
    "  - <=> $max_{p} \\sum_{k=1}^{C} p(k) log(p(k)) + logC$\n",
    "  - <=> $min_{p} - \\sum_{k=1}^{C} p(k) log(p(k))$\n",
    "  - <=> minimize entropy\n",
    "\n",
    "### 5.5 Gain Ratio (C4.5)\n",
    "\n",
    "- to avoid overfitting\n",
    "  - especially, data set has high cardinality categorical variable\n",
    "- penalizes splitting with many values.\n",
    "- split-info = $-\\sum_{j=1}^{K} p(j) log_{2}(p(j))$\n",
    "  - k - number of partition\n",
    "  - p(j) - probability in partion j\n",
    "- GainRatio(A) = $\\frac{information-gain(A)}{SplitInfo(A)}$\n",
    "\n",
    "### 5.6 Gini Index\n",
    "\n",
    "- purity of dataset\n",
    "- Gini(D) = 1 - $\\sum_{j=1}^{C} p(j)^2$\n",
    "  - p(j) - proportion of samples belonging to class j in D\n",
    "- gini index = 1 minus (sum of all squared probability for each label in one node)\n",
    "- the smaller gini, the more purity in D\n",
    "- the larger gini, the less pure D is.\n",
    "\n",
    "#### 5.6.1 CART algorithm and gini reduction \n",
    "\n",
    "**cart**\n",
    "\n",
    "- always splits node into two child nodes.\n",
    "- splitting criteria:\n",
    "  - choose the attribute and condition so that maximized the reduction of gini index\n",
    "  - calculate the gini index before and after split >> check the difference.\n",
    "\n",
    "1. compute the gini index for data set D\n",
    "2. for each value v of attribute A\n",
    "   a. split D into two subsets: D1 (records where A = v) and D2 (records where A≠v)\n",
    "   b. compute the weighted gini index for split\n",
    "\n",
    "   - weighted gini = gini(D1) * abs(D1 / D) + gini(D2) * abs(D2/D)\n",
    "   - weigted gini = gini from spliting\n",
    "   - want to maximize gini(before split) - weighted gini\n",
    "\n",
    "#### 5.6.2 Alternative measures for selecting attributes.\n",
    "\n",
    "- classification error rate\n",
    "  - error(D) = 1 - max(p1,p2,p3,...pc)\n",
    "  - error(D) = 1 >>> all elements are classified correctly\n",
    "  - not differentiable\n",
    "  - sensitive enough for tree growing\n",
    "\n",
    "### 5.7 compare metrics for 2 label \n",
    "\n",
    "- entropy(t) = - sum ( p(j/t) * log(p(j/t))\n",
    "- gini(t) = 1 - sum( p(j/t)**2)\n",
    "- error(t) = 1 - max(p(i/t) for all i)\n",
    "\n",
    "-  all of them\n",
    "  -  the higher, the more uncertian\n",
    "  -  the smller, the more certain.\n",
    "\n",
    "### 5.8 Expressiveness\n",
    "\n",
    "- categorical input/categorical output >> well\n",
    "- continuous input / continuous output >> approximation\n",
    "\n",
    "### 5.9 overfitting\n",
    "\n",
    "**how to detect**\n",
    "\n",
    "- compare train metrics and testing metrics\n",
    "\n",
    "**why overfitting happens**\n",
    "\n",
    "- Too much variance in the training data\n",
    "  - training data is not representative\n",
    "- Too much noise in the training data\n",
    "  - noise = some feature values or class labels are incorrect\n",
    " \n",
    "#### 5.9.1 how to deal with overfitting\n",
    "\n",
    "- regularization\n",
    "  - \n",
    "- pruning (specific to tree)\n",
    "  - pre-prunning\n",
    "    - stop creating more node by judging there is no more meaningful data to make choices\n",
    "  - post-prunning\n",
    "    - grow full tree first and remove nodes that seem not to have sufficient evidence.\n",
    "\n",
    "**how to do prune**\n",
    "\n",
    "- all children are leave\n",
    "- check the accuracy with validation set after changing all the items to most freaquent label.\n",
    "\n",
    "\n",
    "- tools\n",
    "  - cross validation\n",
    "  - statistics testing\n",
    "\n",
    "### 5.10 numerical feature\n",
    "\n",
    "- discretization\n",
    "  - with feature engineering at first\n",
    "  - find the threshold that will leads to best resutl while learning training.\n",
    "\n",
    "-  Split on any feature with any threshold\n",
    "  - binary decision (A < v ) or (A >= v)\n",
    "  - \n",
    "\n",
    "### 5.11 missing value\n",
    "\n",
    "## hyper parameter \n",
    "\n",
    "-  the number of split\n",
    "-  max depth of tree\n",
    "-  minimum number of data in one node.\n",
    "-  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e81e76f-92c2-44de-be3d-933613846605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33393b2c-a29f-4d6e-b9fb-47184774961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X,y = data.data, data.target\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
