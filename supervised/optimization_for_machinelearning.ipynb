{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea98ff3-40e3-422f-a49d-79440de3979b",
   "metadata": {},
   "source": [
    "# optimization for machine learning\n",
    "\n",
    "**methods**\n",
    "\n",
    "- Gradient descent\n",
    "- Stochastic Gradient Descent.\n",
    "  - get gradient only for one data set after shuffle and get it.\n",
    "- min batch stochastic gradient descent\n",
    "- \n",
    "\n",
    "\n",
    "**methods for initializing parameter**\n",
    "\n",
    "- linear regression\n",
    "  - zerros\n",
    "  - small random numbers\n",
    "- neural network\n",
    "  - random\n",
    "  - Xavier\n",
    "  - He\n",
    "  - Bias\n",
    "- Embeded layers (NLP or recommended system)\n",
    "  - uniform\n",
    "  - normal\n",
    "- Lecum \n",
    "\n",
    "\n",
    "# momentum \n",
    "\n",
    "https://www.geeksforgeeks.org/optimization-algorithms-in-machine-learning/\n",
    "\n",
    "\n",
    "https://medium.com/@jaleeladejumo/gradient-descent-from-scratch-batch-gradient-descent-stochastic-gradient-descent-and-mini-batch-def681187473"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b94d7-9e0a-444d-af65-291edc74376a",
   "metadata": {},
   "source": [
    "# Optimization method (Gradient Descent Implementations.)\n",
    "\n",
    "use the gradient of loss function and update the value of parameter in iterative way.\n",
    "\n",
    "Loss function = $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\boldsymbol{w}^T\\boldsymbol{x_i})^2$\n",
    "\n",
    "$=(y-Xw)^T(y-xw)/n$\n",
    "\n",
    "Gradient of loss function = $-\\frac{2}{n}X^T(y-Xw)$\n",
    "\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "$w_{t+1} = w_{t} - \\lambda \\nabla L(\\boldsymbol{w}_t)$\n",
    "\n",
    "\n",
    "- uses entire N data points to compute the gradient\n",
    "\n",
    "-  when data set is large -> time consuming \n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "$w_{t+1} = w_{t} - \\lambda \\nabla L_{i}(\\boldsymbol{w}_t)$\n",
    "\n",
    "- choose i th L from all to approximate the gradient\n",
    "\n",
    "-  when data set is large -> need a lot interation to fully cover the data.\n",
    "\n",
    "  \n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "\n",
    "$w_{t+1} = w_{t} - \\lambda \\nabla L_{B}(\\boldsymbol{w}_t)$\n",
    "\n",
    "- choose B subset of L from all to approximate the gradient\n",
    "  - batch size B is a hyper parameter.\n",
    "\n",
    "- this is in the middle of Batch Gradient D\n",
    "\n",
    "\n",
    "**for all gradient**\n",
    "- $\\lambda$ is learning rate\n",
    "  - $\\lambda$ is hyper parameter\n",
    "- t is iterative number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3c712-ca9c-4c67-9598-7d345cb919fd",
   "metadata": {},
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84947bd0-38ee-4316-b2de-4d597289c170",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5269d5-bf02-41c8-9fc6-a8764bd56dde",
   "metadata": {},
   "source": [
    "# more advanced optimization\n",
    "\n",
    "## Adam \n",
    "\n",
    "## RMsprop\n",
    "\n",
    "## Adagrad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
